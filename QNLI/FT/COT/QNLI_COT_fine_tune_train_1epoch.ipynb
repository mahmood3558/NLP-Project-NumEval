{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T10:00:05.674987Z","iopub.status.busy":"2024-01-27T10:00:05.674692Z","iopub.status.idle":"2024-01-27T10:00:26.501775Z","shell.execute_reply":"2024-01-27T10:00:26.500578Z","shell.execute_reply.started":"2024-01-27T10:00:05.674962Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.25.0)\n","Collecting peft\n","  Obtaining dependency information for peft from https://files.pythonhosted.org/packages/8b/1b/aee2a330d050c493642d59ba6af51f3910cb138ea48ede228c84c204a5af/peft-0.7.1-py3-none-any.whl.metadata\n","  Downloading peft-0.7.1-py3-none-any.whl.metadata (25 kB)\n","Collecting bitsandbytes\n","  Obtaining dependency information for bitsandbytes from https://files.pythonhosted.org/packages/9b/63/489ef9cd7a33c1f08f1b2be51d1b511883c5e34591aaa9873b30021cd679/bitsandbytes-0.42.0-py3-none-any.whl.metadata\n","  Downloading bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\n","Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.36.2)\n","Collecting trl\n","  Obtaining dependency information for trl from https://files.pythonhosted.org/packages/61/ae/fb06164af1d535947067492f6db43446d984d1bfa7084f88dcae12ae7b48/trl-0.7.10-py3-none-any.whl.metadata\n","  Downloading trl-0.7.10-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.24.3)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0)\n","Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.20.2)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.1)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.1)\n","Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.11.4)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.0)\n","Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl) (2.1.0)\n","Collecting tyro>=0.5.11 (from trl)\n","  Obtaining dependency information for tyro>=0.5.11 from https://files.pythonhosted.org/packages/b5/d2/0f8812ddc01f602f31489f1141f13100eef7b24f00a14b2fd27d9e8cbc97/tyro-0.7.0-py3-none-any.whl.metadata\n","  Downloading tyro-0.7.0-py3-none-any.whl.metadata (7.7 kB)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2023.12.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.5.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (0.15)\n","Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (13.5.2)\n","Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n","  Obtaining dependency information for shtab>=1.5.6 from https://files.pythonhosted.org/packages/40/ad/7227da64498eaa7abecee4311008f70869e156014b3270cec36e2e70cd31/shtab-1.6.5-py3-none-any.whl.metadata\n","  Downloading shtab-1.6.5-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (11.0.0)\n","Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.3.7)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (2.0.3)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.70.15)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.8.5)\n","Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.18.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.3.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.16.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2023.3)\n","Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2023.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\n","Downloading peft-0.7.1-py3-none-any.whl (168 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hDownloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading trl-0.7.10-py3-none-any.whl (150 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.9/150.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tyro-0.7.0-py3-none-any.whl (79 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.7/79.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading shtab-1.6.5-py3-none-any.whl (13 kB)\n","Installing collected packages: shtab, bitsandbytes, tyro, trl, peft\n","Successfully installed bitsandbytes-0.42.0 peft-0.7.1 shtab-1.6.5 trl-0.7.10 tyro-0.7.0\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install accelerate peft bitsandbytes transformers trl"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T10:00:26.503731Z","iopub.status.busy":"2024-01-27T10:00:26.503421Z","iopub.status.idle":"2024-01-27T10:00:51.284309Z","shell.execute_reply":"2024-01-27T10:00:51.283164Z","shell.execute_reply.started":"2024-01-27T10:00:26.503703Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.24.3)\n","Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\n","Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.3)\n","Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.12.2)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.5)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.20.2)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\n","Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.12.2)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.5.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n","Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Collecting gdown\n","  Obtaining dependency information for gdown from https://files.pythonhosted.org/packages/9e/db/c5dad38733f0995dea4480de38df1b3f6222b77b0dc89aad2402d86e2ff4/gdown-5.0.1-py3-none-any.whl.metadata\n","  Downloading gdown-5.0.1-py3-none-any.whl.metadata (5.6 kB)\n","Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.12.2)\n","Requirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.31.0)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.1)\n","Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2023.11.17)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n","Downloading gdown-5.0.1-py3-none-any.whl (16 kB)\n","Installing collected packages: gdown\n","Successfully installed gdown-5.0.1\n"]}],"source":["!pip install datasets\n","!pip install gdown"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T10:00:51.286244Z","iopub.status.busy":"2024-01-27T10:00:51.285913Z","iopub.status.idle":"2024-01-27T10:01:21.003788Z","shell.execute_reply":"2024-01-27T10:01:21.002847Z","shell.execute_reply.started":"2024-01-27T10:00:51.286214Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["# Import required libraries\n","import textwrap\n","import pandas as pd\n","from datasets import load_dataset\n","\n","import os\n","import json\n","import torch\n","import random\n","import zipfile\n","import transformers\n","import bitsandbytes as bnb\n","\n","from datasets import Dataset\n","from huggingface_hub import HfApi\n","from datasets import load_dataset\n","from huggingface_hub import login\n","from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel, AutoPeftModelForCausalLM,prepare_model_for_int8_training\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, Trainer, TrainingArguments, DataCollatorForLanguageModeling, EarlyStoppingCallback, pipeline, logging, set_seed, TextStreamer , LlamaTokenizer, StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T10:01:21.006670Z","iopub.status.busy":"2024-01-27T10:01:21.006013Z","iopub.status.idle":"2024-01-27T10:01:23.653066Z","shell.execute_reply":"2024-01-27T10:01:23.651924Z","shell.execute_reply.started":"2024-01-27T10:01:21.006639Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading...\n","From (original): https://drive.google.com/uc?id=1e09QNfGnzey42rf0Pwbk_ru3H5OLZo6N\n","From (redirected): https://drive.google.com/uc?id=1e09QNfGnzey42rf0Pwbk_ru3H5OLZo6N&confirm=t&uuid=74244c1c-ca9c-462a-be0b-a7406f2a3ce9\n","To: /kaggle/working/NumEval_Task1.zip\n","100%|█████████████████████████████████████████| 113M/113M [00:00<00:00, 166MB/s]\n"]}],"source":["!gdown --fuzzy -O NumEval_Task1.zip https://drive.google.com/file/d/1e09QNfGnzey42rf0Pwbk_ru3H5OLZo6N/view?usp=sharing"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T10:01:23.654835Z","iopub.status.busy":"2024-01-27T10:01:23.654524Z","iopub.status.idle":"2024-01-27T10:01:26.910907Z","shell.execute_reply":"2024-01-27T10:01:26.910117Z","shell.execute_reply.started":"2024-01-27T10:01:23.654808Z"},"trusted":true},"outputs":[],"source":["zip_file_path = '/kaggle/working/NumEval_Task1.zip'\n","extracted_folder_path = '/kaggle/working/NumEval'\n","\n","with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n","    zip_ref.extractall(extracted_folder_path)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T10:01:26.912520Z","iopub.status.busy":"2024-01-27T10:01:26.912181Z","iopub.status.idle":"2024-01-27T10:01:26.916627Z","shell.execute_reply":"2024-01-27T10:01:26.915704Z","shell.execute_reply.started":"2024-01-27T10:01:26.912493Z"},"trusted":true},"outputs":[],"source":["base_model = \"microsoft/Orca-2-7b\""]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T10:01:26.918012Z","iopub.status.busy":"2024-01-27T10:01:26.917734Z","iopub.status.idle":"2024-01-27T10:01:26.928875Z","shell.execute_reply":"2024-01-27T10:01:26.928185Z","shell.execute_reply.started":"2024-01-27T10:01:26.917988Z"},"trusted":true},"outputs":[],"source":["list_of_file_train_QNLI= ['/kaggle/working/NumEval/NumEval_Task1/QNLI/QNLI-Stress Test/QNLI-Stress Test_train.json']\n","list_of_file_val_QNLI = ['/kaggle/working/NumEval/NumEval_Task1/QNLI/QNLI-Stress Test/QNLI-Stress Test_dev.json']\n","# list_of_file_test_QP_command = ['/kaggle/working/NumEval/NumEval_Task1/QP/Numeracy600K_comment_test.json',]\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T10:01:26.930225Z","iopub.status.busy":"2024-01-27T10:01:26.929947Z","iopub.status.idle":"2024-01-27T10:01:26.943001Z","shell.execute_reply":"2024-01-27T10:01:26.942254Z","shell.execute_reply.started":"2024-01-27T10:01:26.930202Z"},"trusted":true},"outputs":[],"source":["def give_prompt_QNLI(statement1,statement2,Question,\n","                     statement1_char,statement2_char,\n","                     statement1_sci_10E,statement2_sci_10E,\n","                     statement1_sci_10E_char,statement2_sci_10E_char,\n","                     statement1_mask,statement2_mask,answer):\n","\n","\n","    prompt = f\"\"\"\n","Find the number in the statements and then figure out the sentiment of the statement and then think about the Question and choose one of the options in the Question as answer and then say the  answer. know your explanation as value of explain then give me resualt in format: \"{\" Response: answer , Explanation : explain\"}\"\n","### Input:\n","statement1:{statement1}\n","statement2:{statement2}\n","### Question:\n","{Question}\"\"\"\n","\n","    explain = f'''\n","first make numbers of statement1 to char it is equal to {statement1_char} then make numbers of statement2 to char it is equal to {statement2_char}.\n","seconde make statement1 to sci 10E char it is equal to {statement1_sci_10E_char} then make statement2 to sci 10E char it is equal to {statement2_sci_10E_char}.\n","third make statement1 sci 10E char to sci 10E it is equal to {statement1_sci_10E} then make statement2 sci 10E char to sci 10E it is equal to {statement2_sci_10E}.\n","Now we can mask the number and we now the value of ech of them and then we have statement1 is equal to {statement1_mask} and statement2 is equal to {statement2_mask}.\n","So after this steps we must figure out the sentiment of the statements and then think about the Question use the numbers to find out the answer.'''\n","      \n","    return {'prompt':prompt,'response':answer,'explanation':explain}"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T10:01:26.944321Z","iopub.status.busy":"2024-01-27T10:01:26.944052Z","iopub.status.idle":"2024-01-27T10:01:27.066718Z","shell.execute_reply":"2024-01-27T10:01:27.065896Z","shell.execute_reply.started":"2024-01-27T10:01:26.944299Z"},"trusted":true},"outputs":[],"source":["list_of_prompt_train_QNLI = []\n","list_of_prompt_val_QNLI = []\n","for file_name in list_of_file_train_QNLI:\n","    with open(file_name, 'r') as file:\n","        datas = json.load(file)\n","        for data in datas:\n","            list_of_prompt_train_QNLI.append(give_prompt_QNLI(data['statement1'],data['statement2'],data['options'],\n","                                                                    data['statement1_char'],data['statement2_char'],\n","                                                                    data['statement1_sci_10E'],data['statement2_sci_10E'],\n","                                                                    data['statement1_sci_10E_char'],data['statement2_sci_10E_char'],\n","                                                                    data['statement1_mask'],data['statement2_mask'],data['answer']))\n","\n","for file_name in list_of_file_val_QNLI:\n","    with open(file_name, 'r') as file:\n","        datas = json.load(file)\n","        for data in datas:\n","            list_of_prompt_val_QNLI.append(give_prompt_QNLI(data['statement1'],data['statement2'],data['options'],\n","                                                                    data['statement1_char'],data['statement2_char'],\n","                                                                    data['statement1_sci_10E'],data['statement2_sci_10E'],\n","                                                                    data['statement1_sci_10E_char'],data['statement2_sci_10E_char'],\n","                                                                    data['statement1_mask'],data['statement2_mask'],data['answer']))"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T10:01:27.071619Z","iopub.status.busy":"2024-01-27T10:01:27.071306Z","iopub.status.idle":"2024-01-27T10:01:27.078869Z","shell.execute_reply":"2024-01-27T10:01:27.077883Z","shell.execute_reply.started":"2024-01-27T10:01:27.071589Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'prompt': '\\nFind the number in the statements and then figure out the sentiment of the statement and then think about the Question and choose one of the options in the Question as answer and then say the  answer. know your explanation as value of explain then give me resualt in format: \" Response: answer , Explanation : explain\"\\n### Input:\\nstatement1:Shaquan has more than 3 playing cards , each one is ordered by the number on it , but one card is flipped over . They are numbered 8 , 16 , 24 , x , 40\\nstatement2:Shaquan has 5 playing cards , each one is ordered by the number on it , but one card is flipped over .\\nThey are numbered 8 , 16 , 24 , x , 40\\n### Question:\\n Entailment or contradiction or neutral?',\n"," 'response': 'neutral',\n"," 'explanation': '\\nfirst make numbers of statement1 to char it is equal to Shaquan has more than 3 playing cards , each one is ordered by the number on it , but one card is flipped over . They are numbered 8 , 1 6 , 2 4 , x , 4 0 then make numbers of statement2 to char it is equal to Shaquan has 5 playing cards , each one is ordered by the number on it , but one card is flipped over .\\nThey are numbered 8 , 1 6 , 2 4 , x , 4 0.\\nseconde make statement1 to sci 10E char it is equal to Shaquan has more than 3 . 0 0 0 0 0 0 0 0 0 0 E + 0 0 playing cards , each one is ordered by the number on it , but one card is flipped over . They are numbered 8 . 0 0 0 0 0 0 0 0 0 0 E + 0 0 , 1 . 6 0 0 0 0 0 0 0 0 0 E + 0 1 , 2 . 4 0 0 0 0 0 0 0 0 0 E + 0 1 , x , 4 . 0 0 0 0 0 0 0 0 0 0 E + 0 1 then make statement2 to sci 10E char it is equal to Shaquan has 5 . 0 0 0 0 0 0 0 0 0 0 E + 0 0 playing cards , each one is ordered by the number on it , but one card is flipped over .\\nThey are numbered 8 . 0 0 0 0 0 0 0 0 0 0 E + 0 0 , 1 . 6 0 0 0 0 0 0 0 0 0 E + 0 1 , 2 . 4 0 0 0 0 0 0 0 0 0 E + 0 1 , x , 4 . 0 0 0 0 0 0 0 0 0 0 E + 0 1.\\nthird make statement1 sci 10E char to sci 10E it is equal to Shaquan has more than 3.0000000000E+00 playing cards , each one is ordered by the number on it , but one card is flipped over . They are numbered 8.0000000000E+00 , 1.6000000000E+01 , 2.4.0000000000E+0100000000E+01 , x , 4.0000000000E+01 then make statement2 sci 10E char to sci 10E it is equal to Shaquan has 5.0000000000E+00 playing cards , each one is ordered by the number on it , but one card is flipped over .\\nThey are numbered 8.0000000000E+00 , 1.6000000000E+01 , 2.4.0000000000E+0100000000E+01 , x , 4.0000000000E+01.\\nNow we can mask the number and we now the value of ech of them and then we have statement1 is equal to Shaquan has more than [Num] playing cards , each one is ordered by the number on it , but one card is flipped over . They are numbered [Num] , [Num] , [Num] , x , [Num] and statement2 is equal to Shaquan has [Num] playing cards , each one is ordered by the number on it , but one card is flipped over .\\nThey are numbered [Num] , [Num] , [Num] , x , [Num].\\nSo after this steps we must figure out the sentiment of the statements and then think about the Question use the numbers to find out the answer.'}"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["list_of_prompt_val_QNLI[2]"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T10:01:27.080333Z","iopub.status.busy":"2024-01-27T10:01:27.079991Z","iopub.status.idle":"2024-01-27T10:01:27.089125Z","shell.execute_reply":"2024-01-27T10:01:27.088262Z","shell.execute_reply.started":"2024-01-27T10:01:27.080307Z"},"trusted":true},"outputs":[{"data":{"text/plain":["6475"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["len(list_of_prompt_train_QNLI)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T10:01:27.090645Z","iopub.status.busy":"2024-01-27T10:01:27.090342Z","iopub.status.idle":"2024-01-27T10:01:27.099724Z","shell.execute_reply":"2024-01-27T10:01:27.098728Z","shell.execute_reply.started":"2024-01-27T10:01:27.090621Z"},"trusted":true},"outputs":[{"data":{"text/plain":["970"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["len(list_of_prompt_val_QNLI)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T10:01:27.101179Z","iopub.status.busy":"2024-01-27T10:01:27.100835Z","iopub.status.idle":"2024-01-27T10:01:27.109161Z","shell.execute_reply":"2024-01-27T10:01:27.108441Z","shell.execute_reply.started":"2024-01-27T10:01:27.101153Z"},"trusted":true},"outputs":[],"source":["def QNLI_input_model(json_input):\n","    return f\"\"\"{json_input[\"prompt\"]}\n","###explanation:\n","{json_input[\"explanation\"]}\n","### Response:\n","{{response: {json_input[\"response\"]}}}\"\"\"\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T10:01:27.110496Z","iopub.status.busy":"2024-01-27T10:01:27.110211Z","iopub.status.idle":"2024-01-27T10:01:27.146976Z","shell.execute_reply":"2024-01-27T10:01:27.146033Z","shell.execute_reply.started":"2024-01-27T10:01:27.110473Z"},"trusted":true},"outputs":[],"source":["list_train = []\n","list_val = []\n","\n","# Shuffle\n","shuffled_prompt_train = random.sample(list_of_prompt_train_QNLI, len(list_of_prompt_train_QNLI))\n","\n","for item in shuffled_prompt_train:\n","    list_train.append(QNLI_input_model(item))\n"," \n","# Shuffle\n","shuffled_prompt_val = random.sample(list_of_prompt_val_QNLI, len(list_of_prompt_val_QNLI))\n","\n","for item in shuffled_prompt_val:\n","    list_val.append(QNLI_input_model(item))"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T10:01:27.148459Z","iopub.status.busy":"2024-01-27T10:01:27.148168Z","iopub.status.idle":"2024-01-27T10:01:27.154279Z","shell.execute_reply":"2024-01-27T10:01:27.153380Z","shell.execute_reply.started":"2024-01-27T10:01:27.148427Z"},"trusted":true},"outputs":[{"data":{"text/plain":["6475"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["len(list_train)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T10:01:27.155637Z","iopub.status.busy":"2024-01-27T10:01:27.155340Z","iopub.status.idle":"2024-01-27T10:01:27.165014Z","shell.execute_reply":"2024-01-27T10:01:27.164176Z","shell.execute_reply.started":"2024-01-27T10:01:27.155613Z"},"trusted":true},"outputs":[{"data":{"text/plain":["970"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["len(list_val)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T10:01:27.166581Z","iopub.status.busy":"2024-01-27T10:01:27.166237Z","iopub.status.idle":"2024-01-27T10:01:27.242062Z","shell.execute_reply":"2024-01-27T10:01:27.241279Z","shell.execute_reply.started":"2024-01-27T10:01:27.166551Z"},"trusted":true},"outputs":[],"source":["data_dict_train = {\"train\": list_train}\n","data_dict_val = {\"val\": list_val}\n","\n","dataset_tarin = Dataset.from_dict(data_dict_train)\n","dataset_val = Dataset.from_dict(data_dict_val)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T10:01:27.243561Z","iopub.status.busy":"2024-01-27T10:01:27.243207Z","iopub.status.idle":"2024-01-27T10:01:27.250215Z","shell.execute_reply":"2024-01-27T10:01:27.249352Z","shell.execute_reply.started":"2024-01-27T10:01:27.243529Z"},"trusted":true},"outputs":[],"source":["# orca2 attention dimension\n","lora_r = 16\n","\n","# Alpha parameter for orca2 scaling\n","lora_alpha = 64\n","\n","# Dropout probability for orca2 layers\n","lora_dropout = 0.1\n","\n","# Bias\n","bias = \"none\"\n","\n","# Task type\n","task_type = \"CAUSAL_LM\"\n","\n","# Output directory where the model predictions and checkpoints will be stored\n","output_dir = \"./results\"\n","\n","# Batch size per GPU for training\n","per_device_train_batch_size = 20\n","\n","# Number of update steps to accumulate the gradients for\n","gradient_accumulation_steps = 1\n","\n","# Initial learning rate (AdamW optimizer)\n","learning_rate = 2e-4\n","\n","# Optimizer to use\n","# optim = \"paged_adamw_32bl\"\n","optim = \"paged_adamw_32bit\"\n","\n","# Number of training steps (overrides num_train_epochs)\n","max_steps = 100\n","num_train_epochs = 1\n","\n","# Linear warmup steps from 0 to learning_rate\n","warmup_steps = 2\n","\n","# Enable fp16/bf16 training (set bf16 to True with an A100)\n","fp16 = True\n","\n","# Log every X updates steps\n","logging_steps = 1"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T10:01:27.251964Z","iopub.status.busy":"2024-01-27T10:01:27.251515Z","iopub.status.idle":"2024-01-27T10:01:27.261603Z","shell.execute_reply":"2024-01-27T10:01:27.260785Z","shell.execute_reply.started":"2024-01-27T10:01:27.251933Z"},"trusted":true},"outputs":[],"source":["def load_model(model_name, bnb_config):\n","    \"\"\"\n","    Loads model and model tokenizer\n","\n","    :param model_name: Hugging Face model name\n","    :param bnb_config: Bitsandbytes configuration\n","    \"\"\"\n","\n","    # Get number of GPU device and set maximum memory\n","    n_gpus = torch.cuda.device_count()\n","    max_memory = f'{40960}MB'\n","\n","    # Load model\n","    model = AutoModelForCausalLM.from_pretrained(\n","        model_name,\n","        quantization_config = bnb_config,\n","        device_map = \"auto\", # dispatch the model efficiently on the available resources\n","        max_memory = {i: max_memory for i in range(n_gpus)},\n","    )\n","\n","    # Load model tokenizer with the user authentication token\n","    tokenizer = AutoTokenizer.from_pretrained(model_name,add_eos_token=True)\n","\n","    # Set padding token as EOS token\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","    return model, tokenizer"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T10:01:27.264645Z","iopub.status.busy":"2024-01-27T10:01:27.262683Z","iopub.status.idle":"2024-01-27T10:01:27.278526Z","shell.execute_reply":"2024-01-27T10:01:27.277485Z","shell.execute_reply.started":"2024-01-27T10:01:27.264612Z"},"trusted":true},"outputs":[],"source":["# Activate 4-bit precision base model loading\n","load_in_4bit = True\n","\n","# Activate nested quantization for 4-bit base models (double quantization)\n","bnb_4bit_use_double_quant = True\n","\n","# Quantization type (fp4 or nf4)\n","bnb_4bit_quant_type = \"nf4\"\n","\n","# Compute data type for 4-bit base models\n","bnb_4bit_compute_dtype = torch.bfloat16"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T10:01:27.279965Z","iopub.status.busy":"2024-01-27T10:01:27.279677Z","iopub.status.idle":"2024-01-27T10:01:27.292278Z","shell.execute_reply":"2024-01-27T10:01:27.291371Z","shell.execute_reply.started":"2024-01-27T10:01:27.279943Z"},"trusted":true},"outputs":[],"source":["def tokenize(prompt, add_eos_token=True):\n","        # there's probably a way to do this with the tokenizer settings\n","        # but again, gotta move fast\n","        result = tokenizer(\n","            prompt,\n","            truncation=True,\n","            max_length=cutoff_len,\n","            padding=False,\n","            return_tensors=None,\n","        )\n","        if (\n","            result[\"input_ids\"][-1] != tokenizer.eos_token_id\n","            and len(result[\"input_ids\"]) < cutoff_len\n","            and add_eos_token\n","        ):\n","            result[\"input_ids\"].append(tokenizer.eos_token_id)\n","            result[\"attention_mask\"].append(1)\n","\n","        result[\"labels\"] = result[\"input_ids\"].copy()\n","\n","        return result\n","\n","def generate_and_tokenize_prompt(data_point):\n","        keys = data_point.keys()\n","        \n","        for k in keys:\n","            key=k\n","            break\n","        tokenized_full_prompt = tokenize(data_point[k])\n","\n","        return tokenized_full_prompt"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T10:01:27.293794Z","iopub.status.busy":"2024-01-27T10:01:27.293387Z","iopub.status.idle":"2024-01-27T10:01:27.308919Z","shell.execute_reply":"2024-01-27T10:01:27.308088Z","shell.execute_reply.started":"2024-01-27T10:01:27.293761Z"},"trusted":true},"outputs":[],"source":["def create_bnb_config(load_in_4bit, bnb_4bit_use_double_quant, bnb_4bit_quant_type, bnb_4bit_compute_dtype):\n","    \"\"\"\n","    Configures model quantization method using bitsandbytes to speed up training and inference\n","\n","    :param load_in_4bit: Load model in 4-bit precision mode\n","    :param bnb_4bit_use_double_quant: Nested quantization for 4-bit model\n","    :param bnb_4bit_quant_type: Quantization data type for 4-bit model\n","    :param bnb_4bit_compute_dtype: Computation data type for 4-bit model\n","    \"\"\"\n","\n","    bnb_config = BitsAndBytesConfig(\n","        load_in_4bit = load_in_4bit,\n","        bnb_4bit_use_double_quant = bnb_4bit_use_double_quant,\n","        bnb_4bit_quant_type = bnb_4bit_quant_type,\n","        bnb_4bit_compute_dtype = bnb_4bit_compute_dtype,\n","    )\n","\n","    return bnb_config"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T10:01:27.310432Z","iopub.status.busy":"2024-01-27T10:01:27.310125Z","iopub.status.idle":"2024-01-27T10:05:46.321535Z","shell.execute_reply":"2024-01-27T10:05:46.320552Z","shell.execute_reply.started":"2024-01-27T10:01:27.310388Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c3b0cc198df54a319ac3937ba983b084","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/582 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"14a7b7085eb64fb98f3ad68912171bd8","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8cbaad2d82e24560985e472a4562e087","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0d1fe2e3eeb84cadac210e571c439f2c","version_major":2,"version_minor":0},"text/plain":["pytorch_model-00001-of-00003.bin:   0%|          | 0.00/9.88G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8acc395e855a4e779c6478e3f6a6f794","version_major":2,"version_minor":0},"text/plain":["pytorch_model-00002-of-00003.bin:   0%|          | 0.00/9.89G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7ed2397247664d1787884e6194100565","version_major":2,"version_minor":0},"text/plain":["pytorch_model-00003-of-00003.bin:   0%|          | 0.00/7.18G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7de9b830ec004a3fad9f13d0b2bd1b61","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4938bb242af84bd28ebd6598ed329799","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/146 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"afc65ac20c2840bb8ae601da1fffaee3","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/828 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"97af500b28e64525ab135b7b4e869e3a","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6effb11ae18d421fba96516b66fcdbdc","version_major":2,"version_minor":0},"text/plain":["added_tokens.json:   0%|          | 0.00/69.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"238de6d8328541188cbbb4abe8743a83","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["bnb_config = create_bnb_config(load_in_4bit, bnb_4bit_use_double_quant, bnb_4bit_quant_type, bnb_4bit_compute_dtype)\n","\n","model, tokenizer = load_model(base_model, bnb_config)\n","\n","tokenizer.pad_token_id = 0 \n","tokenizer.padding_side = \"left\" \n","cutoff_len = 512"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T10:05:46.323262Z","iopub.status.busy":"2024-01-27T10:05:46.322882Z","iopub.status.idle":"2024-01-27T10:06:04.010212Z","shell.execute_reply":"2024-01-27T10:06:04.009415Z","shell.execute_reply.started":"2024-01-27T10:05:46.323227Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5c890933d401446f8e15fad956989835","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/6475 [00:00<?, ?ex/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cf0fbb7d9ec54ae4b768415002cfb2b9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/970 [00:00<?, ?ex/s]"]},"metadata":{},"output_type":"display_data"}],"source":["train_data = dataset_tarin.shuffle().map(generate_and_tokenize_prompt)\n","val_data = dataset_val.shuffle().map(generate_and_tokenize_prompt)"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T10:06:04.011859Z","iopub.status.busy":"2024-01-27T10:06:04.011504Z","iopub.status.idle":"2024-01-27T10:06:04.020471Z","shell.execute_reply":"2024-01-27T10:06:04.019427Z","shell.execute_reply.started":"2024-01-27T10:06:04.011827Z"},"trusted":true},"outputs":[],"source":["def find_all_linear_names(model):\n","    \"\"\"\n","    Find modules to apply orca2 to.\n","\n","    :param model: PEFT model\n","    \"\"\"\n","\n","    cls = bnb.nn.Linear4bit\n","    lora_module_names = set()\n","    for name, module in model.named_modules():\n","        if isinstance(module, cls):\n","            names = name.split('.')\n","            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n","\n","    if 'lm_head' in lora_module_names:\n","        lora_module_names.remove('lm_head')\n","    print(f\"orca2 module names: {list(lora_module_names)}\")\n","    return list(lora_module_names)"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T10:06:04.021871Z","iopub.status.busy":"2024-01-27T10:06:04.021559Z","iopub.status.idle":"2024-01-27T10:06:04.032900Z","shell.execute_reply":"2024-01-27T10:06:04.032103Z","shell.execute_reply.started":"2024-01-27T10:06:04.021848Z"},"trusted":true},"outputs":[],"source":["def print_trainable_parameters(model, use_4bit = False):\n","    \"\"\"\n","    Prints the number of trainable parameters in the model.\n","\n","    :param model: PEFT model\n","    \"\"\"\n","\n","    trainable_params = 0\n","    all_param = 0\n","\n","    for _, param in model.named_parameters():\n","        num_params = param.numel()\n","        if num_params == 0 and hasattr(param, \"ds_numel\"):\n","            num_params = param.ds_numel\n","        all_param += num_params\n","        if param.requires_grad:\n","            trainable_params += num_params\n","\n","    if use_4bit:\n","        trainable_params /= 2\n","\n","    print(\n","        f\"All Parameters: {all_param:,d} || Trainable Parameters: {trainable_params:,d} || Trainable Parameters %: {100 * trainable_params / all_param}\"\n","    )"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T10:06:04.034513Z","iopub.status.busy":"2024-01-27T10:06:04.034172Z","iopub.status.idle":"2024-01-27T10:06:04.043250Z","shell.execute_reply":"2024-01-27T10:06:04.042580Z","shell.execute_reply.started":"2024-01-27T10:06:04.034483Z"},"trusted":true},"outputs":[],"source":["def create_peft_config(r, lora_alpha, target_modules, lora_dropout, bias, task_type):\n","    \"\"\"\n","    Creates Parameter-Efficient Fine-Tuning configuration for the model\n","\n","    :param r: orca2 attention dimension\n","    :param lora_alpha: Alpha parameter for orca2 scaling\n","    :param modules: Names of the modules to apply orca2 to\n","    :param lora_dropout: Dropout Probability for orca2 layers\n","    :param bias: Specifies if the bias parameters should be trained\n","    \"\"\"\n","    config = LoraConfig(\n","        r = r,\n","        lora_alpha = lora_alpha,\n","        target_modules = target_modules,\n","        lora_dropout = lora_dropout,\n","        bias = bias,\n","        task_type = task_type,\n","    )\n","\n","    return config"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T10:06:04.049609Z","iopub.status.busy":"2024-01-27T10:06:04.049305Z","iopub.status.idle":"2024-01-27T10:06:04.062420Z","shell.execute_reply":"2024-01-27T10:06:04.061721Z","shell.execute_reply.started":"2024-01-27T10:06:04.049586Z"},"trusted":true},"outputs":[],"source":["def fine_tune(model, tokenizer, dataset, eval_dataset, lora_r, lora_alpha, lora_dropout, bias, task_type, per_device_train_batch_size, gradient_accumulation_steps, warmup_steps, max_steps, num_train_epochs, learning_rate, fp16, logging_steps, output_dir, optim):\n","\n","    \"\"\"\n","    Prepares and fine-tune the pre-trained model.\n","\n","    :param model: Pre-trained Hugging Face model\n","    :param tokenizer: Model tokenizer\n","    :param dataset: Preprocessed training dataset\n","    \"\"\"\n","\n","    # Enable gradient checkpointing to reduce memory usage during fine-tuning\n","    model.gradient_checkpointing_enable()\n","\n","    # Prepare the model for training\n","    model = prepare_model_for_kbit_training(model)\n","\n","    # Get orca2 module names\n","    target_modules = find_all_linear_names(model)\n","\n","    # Create PEFT configuration for these modules and wrap the model to PEFT\n","    peft_config = create_peft_config(lora_r, lora_alpha, target_modules, lora_dropout, bias, task_type)\n","    model = get_peft_model(model, peft_config)\n","\n","    # Print information about the percentage of trainable parameters\n","    print_trainable_parameters(model)\n","\n","    # Training parameters\n","    trainer = Trainer(\n","        model = model,\n","        train_dataset = dataset,\n","        eval_dataset=val_data,\n","        args = TrainingArguments(\n","            per_device_train_batch_size = per_device_train_batch_size,\n","            gradient_accumulation_steps = gradient_accumulation_steps,\n","            warmup_steps = warmup_steps,\n","#             max_steps = max_steps,\n","            num_train_epochs = num_train_epochs,\n","            learning_rate = learning_rate,\n","            fp16 = fp16,\n","            logging_steps = logging_steps,\n","            output_dir = output_dir,\n","            optim = optim,\n","        ),\n","        data_collator = transformers.DataCollatorForSeq2Seq(\n","            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True)\n","        )\n","    model.config.use_cache = False\n","\n","    do_train = True\n","\n","    # Launch training and log metrics\n","    print(\"Training...\")\n","\n","    trainer.train()\n","    model.save_pretrained(output_dir)\n","    torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T10:06:04.063796Z","iopub.status.busy":"2024-01-27T10:06:04.063517Z","iopub.status.idle":"2024-01-27T13:19:26.802050Z","shell.execute_reply":"2024-01-27T13:19:26.800884Z","shell.execute_reply.started":"2024-01-27T10:06:04.063773Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["LoRA module names: ['k_proj', 'gate_proj', 'o_proj', 'v_proj', 'q_proj', 'down_proj', 'up_proj']\n","All Parameters: 3,540,414,464 || Trainable Parameters: 39,976,960 || Trainable Parameters %: 1.1291604530062163\n","Training...\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["Tracking run with wandb version 0.16.2"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240127_100826-8jgdl52r</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/xai_iust/huggingface/runs/8jgdl52r' target=\"_blank\">dandy-shape-18</a></strong> to <a href='https://wandb.ai/xai_iust/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/xai_iust/huggingface' target=\"_blank\">https://wandb.ai/xai_iust/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/xai_iust/huggingface/runs/8jgdl52r' target=\"_blank\">https://wandb.ai/xai_iust/huggingface/runs/8jgdl52r</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='324' max='324' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [324/324 3:09:53, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.846700</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>1.884200</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1.556800</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>1.211900</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>1.001600</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.753100</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.550900</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.407700</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.353600</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.290400</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>0.267800</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>0.284400</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>0.255700</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>0.255400</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.271500</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>0.242100</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>0.216500</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>0.225500</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>0.202200</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.233900</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>0.196800</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>0.198200</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>0.186600</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>0.198200</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.194300</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>0.163700</td>\n","    </tr>\n","    <tr>\n","      <td>27</td>\n","      <td>0.178000</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>0.193200</td>\n","    </tr>\n","    <tr>\n","      <td>29</td>\n","      <td>0.211700</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.176400</td>\n","    </tr>\n","    <tr>\n","      <td>31</td>\n","      <td>0.158700</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>0.164700</td>\n","    </tr>\n","    <tr>\n","      <td>33</td>\n","      <td>0.195600</td>\n","    </tr>\n","    <tr>\n","      <td>34</td>\n","      <td>0.156900</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.161000</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>0.152900</td>\n","    </tr>\n","    <tr>\n","      <td>37</td>\n","      <td>0.199500</td>\n","    </tr>\n","    <tr>\n","      <td>38</td>\n","      <td>0.146400</td>\n","    </tr>\n","    <tr>\n","      <td>39</td>\n","      <td>0.185500</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.150100</td>\n","    </tr>\n","    <tr>\n","      <td>41</td>\n","      <td>0.171200</td>\n","    </tr>\n","    <tr>\n","      <td>42</td>\n","      <td>0.162900</td>\n","    </tr>\n","    <tr>\n","      <td>43</td>\n","      <td>0.147900</td>\n","    </tr>\n","    <tr>\n","      <td>44</td>\n","      <td>0.149000</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.163700</td>\n","    </tr>\n","    <tr>\n","      <td>46</td>\n","      <td>0.158900</td>\n","    </tr>\n","    <tr>\n","      <td>47</td>\n","      <td>0.155200</td>\n","    </tr>\n","    <tr>\n","      <td>48</td>\n","      <td>0.160700</td>\n","    </tr>\n","    <tr>\n","      <td>49</td>\n","      <td>0.132400</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.142000</td>\n","    </tr>\n","    <tr>\n","      <td>51</td>\n","      <td>0.119700</td>\n","    </tr>\n","    <tr>\n","      <td>52</td>\n","      <td>0.172300</td>\n","    </tr>\n","    <tr>\n","      <td>53</td>\n","      <td>0.138200</td>\n","    </tr>\n","    <tr>\n","      <td>54</td>\n","      <td>0.142700</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.153800</td>\n","    </tr>\n","    <tr>\n","      <td>56</td>\n","      <td>0.131200</td>\n","    </tr>\n","    <tr>\n","      <td>57</td>\n","      <td>0.155300</td>\n","    </tr>\n","    <tr>\n","      <td>58</td>\n","      <td>0.129600</td>\n","    </tr>\n","    <tr>\n","      <td>59</td>\n","      <td>0.132500</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.135100</td>\n","    </tr>\n","    <tr>\n","      <td>61</td>\n","      <td>0.122300</td>\n","    </tr>\n","    <tr>\n","      <td>62</td>\n","      <td>0.146800</td>\n","    </tr>\n","    <tr>\n","      <td>63</td>\n","      <td>0.148800</td>\n","    </tr>\n","    <tr>\n","      <td>64</td>\n","      <td>0.156900</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.168300</td>\n","    </tr>\n","    <tr>\n","      <td>66</td>\n","      <td>0.133300</td>\n","    </tr>\n","    <tr>\n","      <td>67</td>\n","      <td>0.116300</td>\n","    </tr>\n","    <tr>\n","      <td>68</td>\n","      <td>0.140700</td>\n","    </tr>\n","    <tr>\n","      <td>69</td>\n","      <td>0.123800</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.134600</td>\n","    </tr>\n","    <tr>\n","      <td>71</td>\n","      <td>0.138700</td>\n","    </tr>\n","    <tr>\n","      <td>72</td>\n","      <td>0.144700</td>\n","    </tr>\n","    <tr>\n","      <td>73</td>\n","      <td>0.131100</td>\n","    </tr>\n","    <tr>\n","      <td>74</td>\n","      <td>0.127800</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.132300</td>\n","    </tr>\n","    <tr>\n","      <td>76</td>\n","      <td>0.143900</td>\n","    </tr>\n","    <tr>\n","      <td>77</td>\n","      <td>0.123000</td>\n","    </tr>\n","    <tr>\n","      <td>78</td>\n","      <td>0.130600</td>\n","    </tr>\n","    <tr>\n","      <td>79</td>\n","      <td>0.143400</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.152400</td>\n","    </tr>\n","    <tr>\n","      <td>81</td>\n","      <td>0.140000</td>\n","    </tr>\n","    <tr>\n","      <td>82</td>\n","      <td>0.124300</td>\n","    </tr>\n","    <tr>\n","      <td>83</td>\n","      <td>0.139300</td>\n","    </tr>\n","    <tr>\n","      <td>84</td>\n","      <td>0.116600</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.140800</td>\n","    </tr>\n","    <tr>\n","      <td>86</td>\n","      <td>0.163200</td>\n","    </tr>\n","    <tr>\n","      <td>87</td>\n","      <td>0.112100</td>\n","    </tr>\n","    <tr>\n","      <td>88</td>\n","      <td>0.141900</td>\n","    </tr>\n","    <tr>\n","      <td>89</td>\n","      <td>0.106500</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.154200</td>\n","    </tr>\n","    <tr>\n","      <td>91</td>\n","      <td>0.142900</td>\n","    </tr>\n","    <tr>\n","      <td>92</td>\n","      <td>0.131000</td>\n","    </tr>\n","    <tr>\n","      <td>93</td>\n","      <td>0.145400</td>\n","    </tr>\n","    <tr>\n","      <td>94</td>\n","      <td>0.137900</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.142600</td>\n","    </tr>\n","    <tr>\n","      <td>96</td>\n","      <td>0.119400</td>\n","    </tr>\n","    <tr>\n","      <td>97</td>\n","      <td>0.120800</td>\n","    </tr>\n","    <tr>\n","      <td>98</td>\n","      <td>0.109000</td>\n","    </tr>\n","    <tr>\n","      <td>99</td>\n","      <td>0.122300</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.146400</td>\n","    </tr>\n","    <tr>\n","      <td>101</td>\n","      <td>0.129300</td>\n","    </tr>\n","    <tr>\n","      <td>102</td>\n","      <td>0.132000</td>\n","    </tr>\n","    <tr>\n","      <td>103</td>\n","      <td>0.133300</td>\n","    </tr>\n","    <tr>\n","      <td>104</td>\n","      <td>0.136900</td>\n","    </tr>\n","    <tr>\n","      <td>105</td>\n","      <td>0.122000</td>\n","    </tr>\n","    <tr>\n","      <td>106</td>\n","      <td>0.155500</td>\n","    </tr>\n","    <tr>\n","      <td>107</td>\n","      <td>0.125700</td>\n","    </tr>\n","    <tr>\n","      <td>108</td>\n","      <td>0.099700</td>\n","    </tr>\n","    <tr>\n","      <td>109</td>\n","      <td>0.137400</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.115100</td>\n","    </tr>\n","    <tr>\n","      <td>111</td>\n","      <td>0.124700</td>\n","    </tr>\n","    <tr>\n","      <td>112</td>\n","      <td>0.145300</td>\n","    </tr>\n","    <tr>\n","      <td>113</td>\n","      <td>0.098700</td>\n","    </tr>\n","    <tr>\n","      <td>114</td>\n","      <td>0.129400</td>\n","    </tr>\n","    <tr>\n","      <td>115</td>\n","      <td>0.102700</td>\n","    </tr>\n","    <tr>\n","      <td>116</td>\n","      <td>0.125400</td>\n","    </tr>\n","    <tr>\n","      <td>117</td>\n","      <td>0.137600</td>\n","    </tr>\n","    <tr>\n","      <td>118</td>\n","      <td>0.089700</td>\n","    </tr>\n","    <tr>\n","      <td>119</td>\n","      <td>0.123500</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.103600</td>\n","    </tr>\n","    <tr>\n","      <td>121</td>\n","      <td>0.110300</td>\n","    </tr>\n","    <tr>\n","      <td>122</td>\n","      <td>0.114000</td>\n","    </tr>\n","    <tr>\n","      <td>123</td>\n","      <td>0.112000</td>\n","    </tr>\n","    <tr>\n","      <td>124</td>\n","      <td>0.093300</td>\n","    </tr>\n","    <tr>\n","      <td>125</td>\n","      <td>0.161200</td>\n","    </tr>\n","    <tr>\n","      <td>126</td>\n","      <td>0.127600</td>\n","    </tr>\n","    <tr>\n","      <td>127</td>\n","      <td>0.104300</td>\n","    </tr>\n","    <tr>\n","      <td>128</td>\n","      <td>0.144500</td>\n","    </tr>\n","    <tr>\n","      <td>129</td>\n","      <td>0.107800</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>0.090600</td>\n","    </tr>\n","    <tr>\n","      <td>131</td>\n","      <td>0.105600</td>\n","    </tr>\n","    <tr>\n","      <td>132</td>\n","      <td>0.130400</td>\n","    </tr>\n","    <tr>\n","      <td>133</td>\n","      <td>0.101800</td>\n","    </tr>\n","    <tr>\n","      <td>134</td>\n","      <td>0.121400</td>\n","    </tr>\n","    <tr>\n","      <td>135</td>\n","      <td>0.116700</td>\n","    </tr>\n","    <tr>\n","      <td>136</td>\n","      <td>0.104600</td>\n","    </tr>\n","    <tr>\n","      <td>137</td>\n","      <td>0.094800</td>\n","    </tr>\n","    <tr>\n","      <td>138</td>\n","      <td>0.126800</td>\n","    </tr>\n","    <tr>\n","      <td>139</td>\n","      <td>0.130500</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>0.120000</td>\n","    </tr>\n","    <tr>\n","      <td>141</td>\n","      <td>0.086800</td>\n","    </tr>\n","    <tr>\n","      <td>142</td>\n","      <td>0.133100</td>\n","    </tr>\n","    <tr>\n","      <td>143</td>\n","      <td>0.100500</td>\n","    </tr>\n","    <tr>\n","      <td>144</td>\n","      <td>0.094400</td>\n","    </tr>\n","    <tr>\n","      <td>145</td>\n","      <td>0.105700</td>\n","    </tr>\n","    <tr>\n","      <td>146</td>\n","      <td>0.126600</td>\n","    </tr>\n","    <tr>\n","      <td>147</td>\n","      <td>0.114500</td>\n","    </tr>\n","    <tr>\n","      <td>148</td>\n","      <td>0.108000</td>\n","    </tr>\n","    <tr>\n","      <td>149</td>\n","      <td>0.119000</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.139100</td>\n","    </tr>\n","    <tr>\n","      <td>151</td>\n","      <td>0.099200</td>\n","    </tr>\n","    <tr>\n","      <td>152</td>\n","      <td>0.096600</td>\n","    </tr>\n","    <tr>\n","      <td>153</td>\n","      <td>0.090100</td>\n","    </tr>\n","    <tr>\n","      <td>154</td>\n","      <td>0.115000</td>\n","    </tr>\n","    <tr>\n","      <td>155</td>\n","      <td>0.087500</td>\n","    </tr>\n","    <tr>\n","      <td>156</td>\n","      <td>0.123900</td>\n","    </tr>\n","    <tr>\n","      <td>157</td>\n","      <td>0.117400</td>\n","    </tr>\n","    <tr>\n","      <td>158</td>\n","      <td>0.091800</td>\n","    </tr>\n","    <tr>\n","      <td>159</td>\n","      <td>0.100400</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>0.087800</td>\n","    </tr>\n","    <tr>\n","      <td>161</td>\n","      <td>0.099400</td>\n","    </tr>\n","    <tr>\n","      <td>162</td>\n","      <td>0.106700</td>\n","    </tr>\n","    <tr>\n","      <td>163</td>\n","      <td>0.095100</td>\n","    </tr>\n","    <tr>\n","      <td>164</td>\n","      <td>0.087400</td>\n","    </tr>\n","    <tr>\n","      <td>165</td>\n","      <td>0.097100</td>\n","    </tr>\n","    <tr>\n","      <td>166</td>\n","      <td>0.104900</td>\n","    </tr>\n","    <tr>\n","      <td>167</td>\n","      <td>0.100800</td>\n","    </tr>\n","    <tr>\n","      <td>168</td>\n","      <td>0.108600</td>\n","    </tr>\n","    <tr>\n","      <td>169</td>\n","      <td>0.104700</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>0.119600</td>\n","    </tr>\n","    <tr>\n","      <td>171</td>\n","      <td>0.101400</td>\n","    </tr>\n","    <tr>\n","      <td>172</td>\n","      <td>0.096700</td>\n","    </tr>\n","    <tr>\n","      <td>173</td>\n","      <td>0.132300</td>\n","    </tr>\n","    <tr>\n","      <td>174</td>\n","      <td>0.097700</td>\n","    </tr>\n","    <tr>\n","      <td>175</td>\n","      <td>0.096300</td>\n","    </tr>\n","    <tr>\n","      <td>176</td>\n","      <td>0.085900</td>\n","    </tr>\n","    <tr>\n","      <td>177</td>\n","      <td>0.108500</td>\n","    </tr>\n","    <tr>\n","      <td>178</td>\n","      <td>0.075400</td>\n","    </tr>\n","    <tr>\n","      <td>179</td>\n","      <td>0.085400</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>0.106200</td>\n","    </tr>\n","    <tr>\n","      <td>181</td>\n","      <td>0.097200</td>\n","    </tr>\n","    <tr>\n","      <td>182</td>\n","      <td>0.095000</td>\n","    </tr>\n","    <tr>\n","      <td>183</td>\n","      <td>0.085500</td>\n","    </tr>\n","    <tr>\n","      <td>184</td>\n","      <td>0.117700</td>\n","    </tr>\n","    <tr>\n","      <td>185</td>\n","      <td>0.079800</td>\n","    </tr>\n","    <tr>\n","      <td>186</td>\n","      <td>0.102200</td>\n","    </tr>\n","    <tr>\n","      <td>187</td>\n","      <td>0.102400</td>\n","    </tr>\n","    <tr>\n","      <td>188</td>\n","      <td>0.074500</td>\n","    </tr>\n","    <tr>\n","      <td>189</td>\n","      <td>0.098000</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>0.084900</td>\n","    </tr>\n","    <tr>\n","      <td>191</td>\n","      <td>0.102200</td>\n","    </tr>\n","    <tr>\n","      <td>192</td>\n","      <td>0.102500</td>\n","    </tr>\n","    <tr>\n","      <td>193</td>\n","      <td>0.057300</td>\n","    </tr>\n","    <tr>\n","      <td>194</td>\n","      <td>0.080100</td>\n","    </tr>\n","    <tr>\n","      <td>195</td>\n","      <td>0.096100</td>\n","    </tr>\n","    <tr>\n","      <td>196</td>\n","      <td>0.117900</td>\n","    </tr>\n","    <tr>\n","      <td>197</td>\n","      <td>0.096600</td>\n","    </tr>\n","    <tr>\n","      <td>198</td>\n","      <td>0.074700</td>\n","    </tr>\n","    <tr>\n","      <td>199</td>\n","      <td>0.104400</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.111200</td>\n","    </tr>\n","    <tr>\n","      <td>201</td>\n","      <td>0.092500</td>\n","    </tr>\n","    <tr>\n","      <td>202</td>\n","      <td>0.081600</td>\n","    </tr>\n","    <tr>\n","      <td>203</td>\n","      <td>0.093800</td>\n","    </tr>\n","    <tr>\n","      <td>204</td>\n","      <td>0.091000</td>\n","    </tr>\n","    <tr>\n","      <td>205</td>\n","      <td>0.061200</td>\n","    </tr>\n","    <tr>\n","      <td>206</td>\n","      <td>0.119800</td>\n","    </tr>\n","    <tr>\n","      <td>207</td>\n","      <td>0.087900</td>\n","    </tr>\n","    <tr>\n","      <td>208</td>\n","      <td>0.113200</td>\n","    </tr>\n","    <tr>\n","      <td>209</td>\n","      <td>0.072600</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>0.077400</td>\n","    </tr>\n","    <tr>\n","      <td>211</td>\n","      <td>0.076900</td>\n","    </tr>\n","    <tr>\n","      <td>212</td>\n","      <td>0.097500</td>\n","    </tr>\n","    <tr>\n","      <td>213</td>\n","      <td>0.078700</td>\n","    </tr>\n","    <tr>\n","      <td>214</td>\n","      <td>0.075700</td>\n","    </tr>\n","    <tr>\n","      <td>215</td>\n","      <td>0.085000</td>\n","    </tr>\n","    <tr>\n","      <td>216</td>\n","      <td>0.085700</td>\n","    </tr>\n","    <tr>\n","      <td>217</td>\n","      <td>0.120200</td>\n","    </tr>\n","    <tr>\n","      <td>218</td>\n","      <td>0.076100</td>\n","    </tr>\n","    <tr>\n","      <td>219</td>\n","      <td>0.072100</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>0.118200</td>\n","    </tr>\n","    <tr>\n","      <td>221</td>\n","      <td>0.098600</td>\n","    </tr>\n","    <tr>\n","      <td>222</td>\n","      <td>0.103600</td>\n","    </tr>\n","    <tr>\n","      <td>223</td>\n","      <td>0.124300</td>\n","    </tr>\n","    <tr>\n","      <td>224</td>\n","      <td>0.087100</td>\n","    </tr>\n","    <tr>\n","      <td>225</td>\n","      <td>0.113100</td>\n","    </tr>\n","    <tr>\n","      <td>226</td>\n","      <td>0.094000</td>\n","    </tr>\n","    <tr>\n","      <td>227</td>\n","      <td>0.069000</td>\n","    </tr>\n","    <tr>\n","      <td>228</td>\n","      <td>0.107300</td>\n","    </tr>\n","    <tr>\n","      <td>229</td>\n","      <td>0.095200</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>0.079700</td>\n","    </tr>\n","    <tr>\n","      <td>231</td>\n","      <td>0.093900</td>\n","    </tr>\n","    <tr>\n","      <td>232</td>\n","      <td>0.104400</td>\n","    </tr>\n","    <tr>\n","      <td>233</td>\n","      <td>0.105500</td>\n","    </tr>\n","    <tr>\n","      <td>234</td>\n","      <td>0.113200</td>\n","    </tr>\n","    <tr>\n","      <td>235</td>\n","      <td>0.104100</td>\n","    </tr>\n","    <tr>\n","      <td>236</td>\n","      <td>0.075800</td>\n","    </tr>\n","    <tr>\n","      <td>237</td>\n","      <td>0.091000</td>\n","    </tr>\n","    <tr>\n","      <td>238</td>\n","      <td>0.088400</td>\n","    </tr>\n","    <tr>\n","      <td>239</td>\n","      <td>0.077100</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>0.120600</td>\n","    </tr>\n","    <tr>\n","      <td>241</td>\n","      <td>0.094300</td>\n","    </tr>\n","    <tr>\n","      <td>242</td>\n","      <td>0.098000</td>\n","    </tr>\n","    <tr>\n","      <td>243</td>\n","      <td>0.076900</td>\n","    </tr>\n","    <tr>\n","      <td>244</td>\n","      <td>0.112600</td>\n","    </tr>\n","    <tr>\n","      <td>245</td>\n","      <td>0.086900</td>\n","    </tr>\n","    <tr>\n","      <td>246</td>\n","      <td>0.098800</td>\n","    </tr>\n","    <tr>\n","      <td>247</td>\n","      <td>0.108500</td>\n","    </tr>\n","    <tr>\n","      <td>248</td>\n","      <td>0.059900</td>\n","    </tr>\n","    <tr>\n","      <td>249</td>\n","      <td>0.108500</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.099400</td>\n","    </tr>\n","    <tr>\n","      <td>251</td>\n","      <td>0.069700</td>\n","    </tr>\n","    <tr>\n","      <td>252</td>\n","      <td>0.084000</td>\n","    </tr>\n","    <tr>\n","      <td>253</td>\n","      <td>0.093000</td>\n","    </tr>\n","    <tr>\n","      <td>254</td>\n","      <td>0.084300</td>\n","    </tr>\n","    <tr>\n","      <td>255</td>\n","      <td>0.084500</td>\n","    </tr>\n","    <tr>\n","      <td>256</td>\n","      <td>0.109600</td>\n","    </tr>\n","    <tr>\n","      <td>257</td>\n","      <td>0.097900</td>\n","    </tr>\n","    <tr>\n","      <td>258</td>\n","      <td>0.112400</td>\n","    </tr>\n","    <tr>\n","      <td>259</td>\n","      <td>0.102000</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>0.080200</td>\n","    </tr>\n","    <tr>\n","      <td>261</td>\n","      <td>0.098900</td>\n","    </tr>\n","    <tr>\n","      <td>262</td>\n","      <td>0.118700</td>\n","    </tr>\n","    <tr>\n","      <td>263</td>\n","      <td>0.059100</td>\n","    </tr>\n","    <tr>\n","      <td>264</td>\n","      <td>0.093500</td>\n","    </tr>\n","    <tr>\n","      <td>265</td>\n","      <td>0.099900</td>\n","    </tr>\n","    <tr>\n","      <td>266</td>\n","      <td>0.070100</td>\n","    </tr>\n","    <tr>\n","      <td>267</td>\n","      <td>0.083500</td>\n","    </tr>\n","    <tr>\n","      <td>268</td>\n","      <td>0.082400</td>\n","    </tr>\n","    <tr>\n","      <td>269</td>\n","      <td>0.123300</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>0.102100</td>\n","    </tr>\n","    <tr>\n","      <td>271</td>\n","      <td>0.065800</td>\n","    </tr>\n","    <tr>\n","      <td>272</td>\n","      <td>0.072000</td>\n","    </tr>\n","    <tr>\n","      <td>273</td>\n","      <td>0.095600</td>\n","    </tr>\n","    <tr>\n","      <td>274</td>\n","      <td>0.088400</td>\n","    </tr>\n","    <tr>\n","      <td>275</td>\n","      <td>0.084300</td>\n","    </tr>\n","    <tr>\n","      <td>276</td>\n","      <td>0.063000</td>\n","    </tr>\n","    <tr>\n","      <td>277</td>\n","      <td>0.064700</td>\n","    </tr>\n","    <tr>\n","      <td>278</td>\n","      <td>0.098000</td>\n","    </tr>\n","    <tr>\n","      <td>279</td>\n","      <td>0.083300</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>0.072900</td>\n","    </tr>\n","    <tr>\n","      <td>281</td>\n","      <td>0.078100</td>\n","    </tr>\n","    <tr>\n","      <td>282</td>\n","      <td>0.086000</td>\n","    </tr>\n","    <tr>\n","      <td>283</td>\n","      <td>0.102100</td>\n","    </tr>\n","    <tr>\n","      <td>284</td>\n","      <td>0.067100</td>\n","    </tr>\n","    <tr>\n","      <td>285</td>\n","      <td>0.085800</td>\n","    </tr>\n","    <tr>\n","      <td>286</td>\n","      <td>0.071100</td>\n","    </tr>\n","    <tr>\n","      <td>287</td>\n","      <td>0.059400</td>\n","    </tr>\n","    <tr>\n","      <td>288</td>\n","      <td>0.081100</td>\n","    </tr>\n","    <tr>\n","      <td>289</td>\n","      <td>0.082500</td>\n","    </tr>\n","    <tr>\n","      <td>290</td>\n","      <td>0.089000</td>\n","    </tr>\n","    <tr>\n","      <td>291</td>\n","      <td>0.090000</td>\n","    </tr>\n","    <tr>\n","      <td>292</td>\n","      <td>0.083400</td>\n","    </tr>\n","    <tr>\n","      <td>293</td>\n","      <td>0.074400</td>\n","    </tr>\n","    <tr>\n","      <td>294</td>\n","      <td>0.088100</td>\n","    </tr>\n","    <tr>\n","      <td>295</td>\n","      <td>0.075000</td>\n","    </tr>\n","    <tr>\n","      <td>296</td>\n","      <td>0.076600</td>\n","    </tr>\n","    <tr>\n","      <td>297</td>\n","      <td>0.073900</td>\n","    </tr>\n","    <tr>\n","      <td>298</td>\n","      <td>0.083500</td>\n","    </tr>\n","    <tr>\n","      <td>299</td>\n","      <td>0.066500</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.081000</td>\n","    </tr>\n","    <tr>\n","      <td>301</td>\n","      <td>0.093200</td>\n","    </tr>\n","    <tr>\n","      <td>302</td>\n","      <td>0.084700</td>\n","    </tr>\n","    <tr>\n","      <td>303</td>\n","      <td>0.084700</td>\n","    </tr>\n","    <tr>\n","      <td>304</td>\n","      <td>0.061100</td>\n","    </tr>\n","    <tr>\n","      <td>305</td>\n","      <td>0.114300</td>\n","    </tr>\n","    <tr>\n","      <td>306</td>\n","      <td>0.061600</td>\n","    </tr>\n","    <tr>\n","      <td>307</td>\n","      <td>0.084000</td>\n","    </tr>\n","    <tr>\n","      <td>308</td>\n","      <td>0.099300</td>\n","    </tr>\n","    <tr>\n","      <td>309</td>\n","      <td>0.055000</td>\n","    </tr>\n","    <tr>\n","      <td>310</td>\n","      <td>0.082000</td>\n","    </tr>\n","    <tr>\n","      <td>311</td>\n","      <td>0.096600</td>\n","    </tr>\n","    <tr>\n","      <td>312</td>\n","      <td>0.085700</td>\n","    </tr>\n","    <tr>\n","      <td>313</td>\n","      <td>0.081900</td>\n","    </tr>\n","    <tr>\n","      <td>314</td>\n","      <td>0.086900</td>\n","    </tr>\n","    <tr>\n","      <td>315</td>\n","      <td>0.070500</td>\n","    </tr>\n","    <tr>\n","      <td>316</td>\n","      <td>0.068800</td>\n","    </tr>\n","    <tr>\n","      <td>317</td>\n","      <td>0.091000</td>\n","    </tr>\n","    <tr>\n","      <td>318</td>\n","      <td>0.070700</td>\n","    </tr>\n","    <tr>\n","      <td>319</td>\n","      <td>0.105300</td>\n","    </tr>\n","    <tr>\n","      <td>320</td>\n","      <td>0.082700</td>\n","    </tr>\n","    <tr>\n","      <td>321</td>\n","      <td>0.084100</td>\n","    </tr>\n","    <tr>\n","      <td>322</td>\n","      <td>0.109900</td>\n","    </tr>\n","    <tr>\n","      <td>323</td>\n","      <td>0.074400</td>\n","    </tr>\n","    <tr>\n","      <td>324</td>\n","      <td>0.098500</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["fine_tune(model, tokenizer, train_data,val_data,lora_r, lora_alpha, lora_dropout, bias, task_type, per_device_train_batch_size, gradient_accumulation_steps, warmup_steps, max_steps, num_train_epochs, learning_rate, fp16, logging_steps, output_dir, optim)"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T13:21:09.270035Z","iopub.status.busy":"2024-01-27T13:21:09.269633Z","iopub.status.idle":"2024-01-27T13:21:09.299226Z","shell.execute_reply":"2024-01-27T13:21:09.298104Z","shell.execute_reply.started":"2024-01-27T13:21:09.270005Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"31c2260a86914af5ad8962d55102f526","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"]},"metadata":{},"output_type":"display_data"}],"source":["login()"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T13:22:08.119699Z","iopub.status.busy":"2024-01-27T13:22:08.119279Z","iopub.status.idle":"2024-01-27T13:22:14.036456Z","shell.execute_reply":"2024-01-27T13:22:14.035483Z","shell.execute_reply.started":"2024-01-27T13:22:08.119670Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fbe0c56633544bba874902d3f1b730a2","version_major":2,"version_minor":0},"text/plain":["adapter_model.safetensors:   0%|          | 0.00/160M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/Mahmood1998/SEM_EVAL_2024/commit/65f94fc62b26932ce260b92616575576ae2c97b3', commit_message='Upload task7/task1/QNLI/fine_tune/COT_fine_tune/Orca_2_7b_task7-1_QNLI_COT_fine_tune_adapter_1epoch_model.safetensors with huggingface_hub', commit_description='', oid='65f94fc62b26932ce260b92616575576ae2c97b3', pr_url=None, pr_revision=None, pr_num=None)"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["api = HfApi()\n","\n","api.upload_file(\n","    path_or_fileobj=\"/kaggle/working/results/adapter_config.json\",\n","    path_in_repo=\"task7/task1/QNLI/fine_tune/COT_fine_tune/Orca_2_7b_task7-1_QNLI_COT_fine_tune_1epoch_adapter_config.json\",\n","    repo_id=\"Mahmood1998/SEM_EVAL_2024\",\n","    repo_type=\"model\",\n",")\n","\n","api.upload_file(\n","    path_or_fileobj=\"/kaggle/working/results/adapter_model.safetensors\",\n","    path_in_repo=\"task7/task1/QNLI/fine_tune/COT_fine_tune/Orca_2_7b_task7-1_QNLI_COT_fine_tune_adapter_1epoch_model.safetensors\",\n","    repo_id=\"Mahmood1998/SEM_EVAL_2024\",\n","    repo_type=\"model\",\n",")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30636,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
