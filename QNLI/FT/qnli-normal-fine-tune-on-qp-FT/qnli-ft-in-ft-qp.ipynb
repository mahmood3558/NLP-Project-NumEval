{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install accelerate peft bitsandbytes transformers trl","metadata":{"execution":{"iopub.status.busy":"2024-01-25T15:02:35.368464Z","iopub.execute_input":"2024-01-25T15:02:35.368990Z","iopub.status.idle":"2024-01-25T15:02:57.073403Z","shell.execute_reply.started":"2024-01-25T15:02:35.368949Z","shell.execute_reply":"2024-01-25T15:02:57.072061Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.25.0)\nCollecting peft\n  Obtaining dependency information for peft from https://files.pythonhosted.org/packages/8b/1b/aee2a330d050c493642d59ba6af51f3910cb138ea48ede228c84c204a5af/peft-0.7.1-py3-none-any.whl.metadata\n  Downloading peft-0.7.1-py3-none-any.whl.metadata (25 kB)\nCollecting bitsandbytes\n  Obtaining dependency information for bitsandbytes from https://files.pythonhosted.org/packages/9b/63/489ef9cd7a33c1f08f1b2be51d1b511883c5e34591aaa9873b30021cd679/bitsandbytes-0.42.0-py3-none-any.whl.metadata\n  Downloading bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.36.2)\nCollecting trl\n  Obtaining dependency information for trl from https://files.pythonhosted.org/packages/61/ae/fb06164af1d535947067492f6db43446d984d1bfa7084f88dcae12ae7b48/trl-0.7.10-py3-none-any.whl.metadata\n  Downloading trl-0.7.10-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.20.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.1)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.11.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl) (2.1.0)\nCollecting tyro>=0.5.11 (from trl)\n  Obtaining dependency information for tyro>=0.5.11 from https://files.pythonhosted.org/packages/b5/d2/0f8812ddc01f602f31489f1141f13100eef7b24f00a14b2fd27d9e8cbc97/tyro-0.7.0-py3-none-any.whl.metadata\n  Downloading tyro-0.7.0-py3-none-any.whl.metadata (7.7 kB)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (0.15)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (13.5.2)\nCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n  Obtaining dependency information for shtab>=1.5.6 from https://files.pythonhosted.org/packages/40/ad/7227da64498eaa7abecee4311008f70869e156014b3270cec36e2e70cd31/shtab-1.6.5-py3-none-any.whl.metadata\n  Downloading shtab-1.6.5-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (2.0.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.70.15)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.8.5)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.18.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.3.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.16.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2023.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\nDownloading peft-0.7.1-py3-none-any.whl (168 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading trl-0.7.10-py3-none-any.whl (150 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.9/150.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tyro-0.7.0-py3-none-any.whl (79 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.7/79.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading shtab-1.6.5-py3-none-any.whl (13 kB)\nInstalling collected packages: shtab, bitsandbytes, tyro, trl, peft\nSuccessfully installed bitsandbytes-0.42.0 peft-0.7.1 shtab-1.6.5 trl-0.7.10 tyro-0.7.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install datasets\n!pip install gdown","metadata":{"execution":{"iopub.status.busy":"2024-01-25T15:02:57.075598Z","iopub.execute_input":"2024-01-25T15:02:57.075919Z","iopub.status.idle":"2024-01-25T15:03:22.797037Z","shell.execute_reply.started":"2024-01-25T15:02:57.075888Z","shell.execute_reply":"2024-01-25T15:03:22.795960Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.24.3)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.12.2)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.5)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.20.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nCollecting gdown\n  Obtaining dependency information for gdown from https://files.pythonhosted.org/packages/68/fb/c1bb2cfbf1ad068129e3d67f3420649d38183cca7118f4fa46cfe3c3adab/gdown-5.0.0-py3-none-any.whl.metadata\n  Downloading gdown-5.0.0-py3-none-any.whl.metadata (5.6 kB)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.12.2)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.1)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2023.11.17)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nDownloading gdown-5.0.0-py3-none-any.whl (16 kB)\nInstalling collected packages: gdown\nSuccessfully installed gdown-5.0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!gdown --fuzzy -O adapter_config.json https://huggingface.co/Mahmood1998/SEM_EVAL_2024/resolve/main/task7/task1/QP/fine_tune/normal_fine_tune/Orca_2_7b_task7-1_QP_normal_fine_tune_adapter_config_1_epoch_in_20000_item.json?download=true\n!gdown --fuzzy -O adapter_model.safetensors https://huggingface.co/Mahmood1998/SEM_EVAL_2024/resolve/main/task7/task1/QP/fine_tune/normal_fine_tune/Orca_2_7b_task7-1_QP_normal_fine_tune_adapter_model_1_epoch_in_20000_item.safetensors?download=true","metadata":{"execution":{"iopub.status.busy":"2024-01-25T15:03:22.798590Z","iopub.execute_input":"2024-01-25T15:03:22.798932Z","iopub.status.idle":"2024-01-25T15:03:29.994260Z","shell.execute_reply.started":"2024-01-25T15:03:22.798900Z","shell.execute_reply":"2024-01-25T15:03:29.993147Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading...\nFrom: https://huggingface.co/Mahmood1998/SEM_EVAL_2024/resolve/main/task7/task1/QP/fine_tune/normal_fine_tune/Orca_2_7b_task7-1_QP_normal_fine_tune_adapter_config_1_epoch_in_20000_item.json?download=true\nTo: /kaggle/working/adapter_config.json\n100%|██████████████████████████████████████████| 648/648 [00:00<00:00, 3.91MB/s]\nDownloading...\nFrom: https://huggingface.co/Mahmood1998/SEM_EVAL_2024/resolve/main/task7/task1/QP/fine_tune/normal_fine_tune/Orca_2_7b_task7-1_QP_normal_fine_tune_adapter_model_1_epoch_in_20000_item.safetensors?download=true\nTo: /kaggle/working/adapter_model.safetensors\n100%|████████████████████████████████████████| 160M/160M [00:03<00:00, 44.8MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import required libraries\nimport textwrap\nimport pandas as pd\nfrom datasets import load_dataset\n\nimport os\nimport json\nimport torch\nimport random\nimport zipfile\nimport transformers\nimport bitsandbytes as bnb\n\nfrom datasets import Dataset\nfrom huggingface_hub import HfApi\nfrom datasets import load_dataset\nfrom huggingface_hub import login\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel, AutoPeftModelForCausalLM,prepare_model_for_int8_training\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, Trainer, TrainingArguments, DataCollatorForLanguageModeling, EarlyStoppingCallback, pipeline, logging, set_seed, TextStreamer , LlamaTokenizer, StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\n","metadata":{"execution":{"iopub.status.busy":"2024-01-25T15:03:29.995829Z","iopub.execute_input":"2024-01-25T15:03:29.996948Z","iopub.status.idle":"2024-01-25T15:03:58.578003Z","shell.execute_reply.started":"2024-01-25T15:03:29.996867Z","shell.execute_reply":"2024-01-25T15:03:58.577093Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"!gdown --fuzzy -O NumEval_Task1.zip https://drive.google.com/file/d/1e09QNfGnzey42rf0Pwbk_ru3H5OLZo6N/view?usp=sharing","metadata":{"execution":{"iopub.status.busy":"2024-01-25T15:03:58.581403Z","iopub.execute_input":"2024-01-25T15:03:58.582263Z","iopub.status.idle":"2024-01-25T15:04:00.985213Z","shell.execute_reply.started":"2024-01-25T15:03:58.582231Z","shell.execute_reply":"2024-01-25T15:04:00.984137Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Downloading...\nFrom (original): https://drive.google.com/uc?id=1e09QNfGnzey42rf0Pwbk_ru3H5OLZo6N\nFrom (redirected): https://drive.google.com/uc?id=1e09QNfGnzey42rf0Pwbk_ru3H5OLZo6N&confirm=t&uuid=4e57da98-fa3d-4e38-8d1d-99fcd76bdf02\nTo: /kaggle/working/NumEval_Task1.zip\n100%|█████████████████████████████████████████| 113M/113M [00:00<00:00, 184MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"zip_file_path = '/kaggle/working/NumEval_Task1.zip'\nextracted_folder_path = '/kaggle/working/NumEval'\n\nwith zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n    zip_ref.extractall(extracted_folder_path)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T15:04:00.986877Z","iopub.execute_input":"2024-01-25T15:04:00.987266Z","iopub.status.idle":"2024-01-25T15:04:04.426722Z","shell.execute_reply.started":"2024-01-25T15:04:00.987227Z","shell.execute_reply":"2024-01-25T15:04:04.425847Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# base_model = \"FlagAlpha/Llama2-Chinese-7b-Chat\"\nbase_model = \"microsoft/Orca-2-7b\"","metadata":{"execution":{"iopub.status.busy":"2024-01-25T15:04:04.428168Z","iopub.execute_input":"2024-01-25T15:04:04.428613Z","iopub.status.idle":"2024-01-25T15:04:04.433640Z","shell.execute_reply.started":"2024-01-25T15:04:04.428574Z","shell.execute_reply":"2024-01-25T15:04:04.432480Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"list_of_file_train_QNLI= ['/kaggle/working/NumEval/NumEval_Task1/QNLI/QNLI-Stress Test/QNLI-Stress Test_train.json']\nlist_of_file_val_QNLI = ['/kaggle/working/NumEval/NumEval_Task1/QNLI/QNLI-Stress Test/QNLI-Stress Test_dev.json']\n# list_of_file_test_QP_command = ['/kaggle/working/NumEval/NumEval_Task1/QP/Numeracy600K_comment_test.json',]\n","metadata":{"execution":{"iopub.status.busy":"2024-01-25T15:04:04.435177Z","iopub.execute_input":"2024-01-25T15:04:04.435950Z","iopub.status.idle":"2024-01-25T15:04:04.456324Z","shell.execute_reply.started":"2024-01-25T15:04:04.435911Z","shell.execute_reply":"2024-01-25T15:04:04.455101Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def give_prompt_QNLI(statement1, statement2, Question, answer):\n\n    prompt = f\"\"\"\nDetermine the numerical values in the provided statements, assess the sentiment of each statement, consider the associated question, and select an answer option. Respond with only the chosen answer without providing explanations. Present the result in the format: {{ 'Response': 'answer' }}\n### Input:\nstatement1:{statement1}\nstatement2:{statement2}\n### Question:{Question}\"\"\"\n\n#     prompt = f\"\"\"\n# Given the following statements. Respond with only the chosen answer without providing explanations. Present the result in the format: {{ 'Response': 'answer' }}\n# ### Input:\n# statement1:{statement1}\n# statement2:{statement2}\n# ### Determine the relationship between the statements. Is it entailment,:{Question}\"\"\"\n    \n    return {'prompt':prompt,'response':answer}","metadata":{"execution":{"iopub.status.busy":"2024-01-25T15:04:04.457795Z","iopub.execute_input":"2024-01-25T15:04:04.458213Z","iopub.status.idle":"2024-01-25T15:04:04.474964Z","shell.execute_reply.started":"2024-01-25T15:04:04.458174Z","shell.execute_reply":"2024-01-25T15:04:04.474042Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"list_of_prompt_train_QNLI = []\nlist_of_prompt_val_QNLI = []\nfor file_name in list_of_file_train_QNLI:\n    with open(file_name, 'r') as file:\n        data = json.load(file)\n        for item in data:\n            list_of_prompt_train_QNLI.append(give_prompt_QNLI(item['statement1'],item['statement2'],item['options'], item['answer']))\n\nfor file_name in list_of_file_val_QNLI:\n    with open(file_name, 'r') as file:\n        data = json.load(file)\n        for item in data:\n            list_of_prompt_val_QNLI.append(give_prompt_QNLI(item['statement1'],item['statement2'],item['options'], item['answer']))","metadata":{"execution":{"iopub.status.busy":"2024-01-25T15:04:04.476107Z","iopub.execute_input":"2024-01-25T15:04:04.476411Z","iopub.status.idle":"2024-01-25T15:04:04.589932Z","shell.execute_reply.started":"2024-01-25T15:04:04.476377Z","shell.execute_reply":"2024-01-25T15:04:04.588931Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"list_of_prompt_val_QNLI[2]","metadata":{"execution":{"iopub.status.busy":"2024-01-25T15:04:04.591349Z","iopub.execute_input":"2024-01-25T15:04:04.591700Z","iopub.status.idle":"2024-01-25T15:04:04.599242Z","shell.execute_reply.started":"2024-01-25T15:04:04.591672Z","shell.execute_reply":"2024-01-25T15:04:04.598265Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"{'prompt': \"\\nDetermine the numerical values in the provided statements, assess the sentiment of each statement, consider the associated question, and select an answer option. Respond with only the chosen answer without providing explanations. Present the result in the format: { 'Response': 'answer' }\\n### Input:\\nstatement1:Shaquan has more than 3 playing cards , each one is ordered by the number on it , but one card is flipped over . They are numbered 8 , 16 , 24 , x , 40\\nstatement2:Shaquan has 5 playing cards , each one is ordered by the number on it , but one card is flipped over .\\nThey are numbered 8 , 16 , 24 , x , 40\\n### Question: Entailment or contradiction or neutral?\",\n 'response': 'neutral'}"},"metadata":{}}]},{"cell_type":"code","source":"len(list_of_prompt_val_QNLI)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T15:04:04.600790Z","iopub.execute_input":"2024-01-25T15:04:04.601146Z","iopub.status.idle":"2024-01-25T15:04:04.610674Z","shell.execute_reply.started":"2024-01-25T15:04:04.601119Z","shell.execute_reply":"2024-01-25T15:04:04.609647Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"970"},"metadata":{}}]},{"cell_type":"code","source":"# print(len(list_of_json_Train_QP))\n# print(len(list_of_json_Test_QP))\n# print(list_of_json_Train_QP[0])","metadata":{"execution":{"iopub.status.busy":"2024-01-25T15:04:04.611979Z","iopub.execute_input":"2024-01-25T15:04:04.612759Z","iopub.status.idle":"2024-01-25T15:04:04.624724Z","shell.execute_reply.started":"2024-01-25T15:04:04.612724Z","shell.execute_reply":"2024-01-25T15:04:04.623954Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def QNLI_input_model(json_input):\n    return f\"\"\"{json_input[\"prompt\"]}\n\n### Response:\n{{response: {json_input[\"response\"]}}}\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-01-25T15:04:04.629656Z","iopub.execute_input":"2024-01-25T15:04:04.629999Z","iopub.status.idle":"2024-01-25T15:04:04.635328Z","shell.execute_reply.started":"2024-01-25T15:04:04.629972Z","shell.execute_reply":"2024-01-25T15:04:04.634547Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"list_train = []\nlist_val = []\n\n# Shuffle\nshuffled_prompt_train = random.sample(list_of_prompt_train_QNLI, len(list_of_prompt_train_QNLI))\n\nfor item in shuffled_prompt_train:\n    list_train.append(QNLI_input_model(item))\n \n# Shuffle\nshuffled_prompt_val = random.sample(list_of_prompt_val_QNLI, len(list_of_prompt_val_QNLI))\n\nfor item in shuffled_prompt_val:\n    list_val.append(QNLI_input_model(item))","metadata":{"execution":{"iopub.status.busy":"2024-01-25T15:04:04.636345Z","iopub.execute_input":"2024-01-25T15:04:04.636681Z","iopub.status.idle":"2024-01-25T15:04:04.664830Z","shell.execute_reply.started":"2024-01-25T15:04:04.636650Z","shell.execute_reply":"2024-01-25T15:04:04.664089Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"len(list_train)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T15:04:04.665970Z","iopub.execute_input":"2024-01-25T15:04:04.666561Z","iopub.status.idle":"2024-01-25T15:04:04.678640Z","shell.execute_reply.started":"2024-01-25T15:04:04.666530Z","shell.execute_reply":"2024-01-25T15:04:04.677753Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"6475"},"metadata":{}}]},{"cell_type":"code","source":"len(list_val)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T15:04:04.679789Z","iopub.execute_input":"2024-01-25T15:04:04.680486Z","iopub.status.idle":"2024-01-25T15:04:04.689258Z","shell.execute_reply.started":"2024-01-25T15:04:04.680450Z","shell.execute_reply":"2024-01-25T15:04:04.688417Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"970"},"metadata":{}}]},{"cell_type":"code","source":"data_dict_train = {\"train\": list_train}\ndata_dict_val = {\"val\": list_val}\n\ndataset_tarin = Dataset.from_dict(data_dict_train)\ndataset_val = Dataset.from_dict(data_dict_val)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T15:04:04.690454Z","iopub.execute_input":"2024-01-25T15:04:04.690827Z","iopub.status.idle":"2024-01-25T15:04:04.733835Z","shell.execute_reply.started":"2024-01-25T15:04:04.690800Z","shell.execute_reply":"2024-01-25T15:04:04.733139Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# LoRA attention dimension\nlora_r = 16\n\n# Alpha parameter for LoRA scaling\nlora_alpha = 64\n\n# Dropout probability for LoRA layers\nlora_dropout = 0.1\n\n# Bias\nbias = \"none\"\n\n# Task type\ntask_type = \"CAUSAL_LM\"\n\n################################################################################\n# TrainingArguments parameters\n################################################################################\n\n# Output directory where the model predictions and checkpoints will be stored\noutput_dir = \"./results\"\n\n# Batch size per GPU for training\nper_device_train_batch_size = 25\n\n# Number of update steps to accumulate the gradients for\ngradient_accumulation_steps = 1\n\n# Initial learning rate (AdamW optimizer)\nlearning_rate = 2e-4\n\n# Optimizer to use\n# optim = \"paged_adamw_32bl\"\noptim = \"paged_adamw_32bit\"\n\n# Number of training steps (overrides num_train_epochs)\nmax_steps = 100\nnum_train_epochs = 1\n\n# Linear warmup steps from 0 to learning_rate\nwarmup_steps = 2\n\n# Enable fp16/bf16 training (set bf16 to True with an A100)\nfp16 = True\n\n# Log every X updates steps\nlogging_steps = 1","metadata":{"execution":{"iopub.status.busy":"2024-01-25T15:04:04.734863Z","iopub.execute_input":"2024-01-25T15:04:04.735131Z","iopub.status.idle":"2024-01-25T15:04:04.741205Z","shell.execute_reply.started":"2024-01-25T15:04:04.735108Z","shell.execute_reply":"2024-01-25T15:04:04.740207Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Activate 4-bit precision base model loading\nload_in_4bit = True\n\n# Activate nested quantization for 4-bit base models (double quantization)\nbnb_4bit_use_double_quant = True\n\n# Quantization type (fp4 or nf4)\nbnb_4bit_quant_type = \"nf4\"\n\n# Compute data type for 4-bit base models\nbnb_4bit_compute_dtype = torch.bfloat16","metadata":{"execution":{"iopub.status.busy":"2024-01-25T15:04:04.742270Z","iopub.execute_input":"2024-01-25T15:04:04.742640Z","iopub.status.idle":"2024-01-25T15:04:04.753592Z","shell.execute_reply.started":"2024-01-25T15:04:04.742617Z","shell.execute_reply":"2024-01-25T15:04:04.752869Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def create_bnb_config(load_in_4bit, bnb_4bit_use_double_quant, bnb_4bit_quant_type, bnb_4bit_compute_dtype):\n    \"\"\"\n    Configures model quantization method using bitsandbytes to speed up training and inference\n\n    :param load_in_4bit: Load model in 4-bit precision mode\n    :param bnb_4bit_use_double_quant: Nested quantization for 4-bit model\n    :param bnb_4bit_quant_type: Quantization data type for 4-bit model\n    :param bnb_4bit_compute_dtype: Computation data type for 4-bit model\n    \"\"\"\n\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit = load_in_4bit,\n        bnb_4bit_use_double_quant = bnb_4bit_use_double_quant,\n        bnb_4bit_quant_type = bnb_4bit_quant_type,\n        bnb_4bit_compute_dtype = bnb_4bit_compute_dtype,\n    )\n\n    return bnb_config","metadata":{"execution":{"iopub.status.busy":"2024-01-25T15:04:04.754727Z","iopub.execute_input":"2024-01-25T15:04:04.755051Z","iopub.status.idle":"2024-01-25T15:04:04.768059Z","shell.execute_reply.started":"2024-01-25T15:04:04.755021Z","shell.execute_reply":"2024-01-25T15:04:04.767224Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"bnb_config = create_bnb_config(load_in_4bit, bnb_4bit_use_double_quant, bnb_4bit_quant_type, bnb_4bit_compute_dtype)\n\nmodel_name = \"microsoft/Orca-2-7b\"\nadapters_name = \"/kaggle/working/\"\n\nprint(f\"Starting to load the model {model_name} into memory\")\ntokenizer = transformers.AutoTokenizer.from_pretrained(\n        \"microsoft/Orca-2-13b\",\n        use_fast=False,\n    )\n\nmax_memory = f'{int(torch.cuda.mem_get_info()[0]/1024**3)-1}GB'\nn_gpus = torch.cuda.device_count()\nmax_memory = {i: max_memory for i in range(n_gpus)}\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map='auto',\n    load_in_8bit=True,\n    max_memory=max_memory,\n    trust_remote_code=True,\n    quantization_config = bnb_config\n)\n\nmodel = PeftModel.from_pretrained(model, adapters_name)\nmodel = model.merge_and_unload()\ntokenizer.bos_token_id = 1\nstop_token_ids = [0]\n\nprint(f\"Successfully loaded the model {model_name} into memory\")","metadata":{"execution":{"iopub.status.busy":"2024-01-25T15:04:04.769132Z","iopub.execute_input":"2024-01-25T15:04:04.769522Z","iopub.status.idle":"2024-01-25T15:09:03.602183Z","shell.execute_reply.started":"2024-01-25T15:04:04.769446Z","shell.execute_reply":"2024-01-25T15:09:03.600953Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Starting to load the model microsoft/Orca-2-7b into memory\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/828 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73845c5118a74bc5ab2309e1ca40e985"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b858bc9b24b43538cfd25b956db9f8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/69.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee45acabd63e40179d7504515495f840"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e5fdb41bfd14846bd16b69b73796187"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/582 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69d5bb4dcdc74f089e25eeb1a9b019de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef84a4fffbbe4293baa45f9ff69ee06d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df479bb61a614568b24b1f16d9450c3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00003.bin:   0%|          | 0.00/9.88G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8079e531532b4be3ae845cbfd4a121db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00003.bin:   0%|          | 0.00/9.89G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc8144fc966045efa6d86407205bd96b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00003-of-00003.bin:   0%|          | 0.00/7.18G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ec2ba931f664df4a0d8634eafe14102"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37c3a6793d0d4614aa2f689cfb9e7d5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/146 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be852b4adc4b4c96a3a51314ba8e603e"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:229: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Successfully loaded the model microsoft/Orca-2-7b into memory\n","output_type":"stream"}]},{"cell_type":"code","source":"# def load_model(model_name, bnb_config):\n#     adapters_name = \"/kaggle/working/\"\n\n#     print(f\"Starting to load the model {model_name} into memory\")\n#     tokenizer = transformers.AutoTokenizer.from_pretrained(\n#             \"microsoft/Orca-2-13b\",\n#             use_fast=False,\n#         )\n\n#     max_memory = f'{int(torch.cuda.mem_get_info()[0]/1024**3)-1}GB'\n#     n_gpus = torch.cuda.device_count()\n#     max_memory = {i: max_memory for i in range(n_gpus)}\n#     model = AutoModelForCausalLM.from_pretrained(\n#         model_name,\n#         device_map='auto',\n#         load_in_8bit=True,\n#         max_memory=max_memory,\n#         trust_remote_code=True\n#     )\n\n#     model = PeftModel.from_pretrained(model, adapters_name)\n#     model = model.merge_and_unload()\n# #     tokenizer.bos_token_id = 1\n# #     stop_token_ids = [0]\n# #     \"\"\"\n# #     Loads model and model tokenizer\n\n# #     :param model_name: Hugging Face model name\n# #     :param bnb_config: Bitsandbytes configuration\n# #     \"\"\"\n# #     adapters_name = \"/kaggle/working/\"\n# #     # Get number of GPU device and set maximum memory\n# #     n_gpus = torch.cuda.device_count()\n# #     max_memory = f'{int(torch.cuda.mem_get_info()[0]/1024**3)-1}GB'\n# #     max_memory = {i: max_memory for i in range(n_gpus)}\n\n# #     # Load model\n# #     model = AutoModelForCausalLM.from_pretrained(\n# #         model_name,\n# #         device_map='auto',\n# #         load_in_8bit=True,\n# #         max_memory=max_memory,\n# #         trust_remote_code=True\n# #     )\n# #     # Load model tokenizer with the user authentication token\n# #     tokenizer = AutoTokenizer.from_pretrained(model_name,add_eos_token=True)\n\n# #     # print(\"1\")\n# #     model = PeftModel.from_pretrained(model, adapters_name)\n# #     # print(\"2\")\n# #     model = model.merge_and_unload()\n# #     # print(\"3\")\n# #     # tok = LlamaTokenizer.from_pretrained(model_name)\n# #     tokenizer.bos_token_id = 1\n# #     stop_token_ids = [0]\n        \n# #     # Set padding token as EOS token\n# #     tokenizer.pad_token = tokenizer.eos_token\n\n#     return model, tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-01-25T15:09:03.603694Z","iopub.execute_input":"2024-01-25T15:09:03.604103Z","iopub.status.idle":"2024-01-25T15:09:03.610810Z","shell.execute_reply.started":"2024-01-25T15:09:03.604065Z","shell.execute_reply":"2024-01-25T15:09:03.609882Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# # Activate 4-bit precision base model loading\n# load_in_4bit = True\n\n# # Activate nested quantization for 4-bit base models (double quantization)\n# bnb_4bit_use_double_quant = True\n\n# # Quantization type (fp4 or nf4)\n# bnb_4bit_quant_type = \"nf4\"\n\n# # Compute data type for 4-bit base models\n# bnb_4bit_compute_dtype = torch.bfloat16","metadata":{"execution":{"iopub.status.busy":"2024-01-25T15:09:03.612079Z","iopub.execute_input":"2024-01-25T15:09:03.612410Z","iopub.status.idle":"2024-01-25T15:09:03.630215Z","shell.execute_reply.started":"2024-01-25T15:09:03.612366Z","shell.execute_reply":"2024-01-25T15:09:03.629448Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def tokenize(prompt, add_eos_token=True):\n        # there's probably a way to do this with the tokenizer settings\n        # but again, gotta move fast\n        result = tokenizer(\n            prompt,\n            truncation=True,\n            max_length=cutoff_len,\n            padding=False,\n            return_tensors=None,\n        )\n        if (\n            result[\"input_ids\"][-1] != tokenizer.eos_token_id\n            and len(result[\"input_ids\"]) < cutoff_len\n            and add_eos_token\n        ):\n            result[\"input_ids\"].append(tokenizer.eos_token_id)\n            result[\"attention_mask\"].append(1)\n\n        result[\"labels\"] = result[\"input_ids\"].copy()\n\n        return result\n\ndef generate_and_tokenize_prompt(data_point):\n        keys = data_point.keys()\n        \n        for k in keys:\n            key=k\n            break\n        tokenized_full_prompt = tokenize(data_point[k])\n\n        return tokenized_full_prompt","metadata":{"execution":{"iopub.status.busy":"2024-01-25T15:09:03.631370Z","iopub.execute_input":"2024-01-25T15:09:03.631675Z","iopub.status.idle":"2024-01-25T15:09:03.645686Z","shell.execute_reply.started":"2024-01-25T15:09:03.631651Z","shell.execute_reply":"2024-01-25T15:09:03.644930Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# def create_bnb_config(load_in_4bit, bnb_4bit_use_double_quant, bnb_4bit_quant_type, bnb_4bit_compute_dtype):\n#     \"\"\"\n#     Configures model quantization method using bitsandbytes to speed up training and inference\n\n#     :param load_in_4bit: Load model in 4-bit precision mode\n#     :param bnb_4bit_use_double_quant: Nested quantization for 4-bit model\n#     :param bnb_4bit_quant_type: Quantization data type for 4-bit model\n#     :param bnb_4bit_compute_dtype: Computation data type for 4-bit model\n#     \"\"\"\n\n#     bnb_config = BitsAndBytesConfig(\n#         load_in_4bit = load_in_4bit,\n#         bnb_4bit_use_double_quant = bnb_4bit_use_double_quant,\n#         bnb_4bit_quant_type = bnb_4bit_quant_type,\n#         bnb_4bit_compute_dtype = bnb_4bit_compute_dtype,\n#     )\n\n#     return bnb_config","metadata":{"execution":{"iopub.status.busy":"2024-01-25T15:09:03.646761Z","iopub.execute_input":"2024-01-25T15:09:03.647048Z","iopub.status.idle":"2024-01-25T15:09:03.660374Z","shell.execute_reply.started":"2024-01-25T15:09:03.647023Z","shell.execute_reply":"2024-01-25T15:09:03.659403Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# bnb_config = create_bnb_config(load_in_4bit, bnb_4bit_use_double_quant, bnb_4bit_quant_type, bnb_4bit_compute_dtype)\n\n# model, tokenizer = load_model(base_model, bnb_config)\n\ntokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\ntokenizer.padding_side = \"left\"  # Allow batched inference\ncutoff_len = 512","metadata":{"execution":{"iopub.status.busy":"2024-01-25T15:09:03.662314Z","iopub.execute_input":"2024-01-25T15:09:03.663109Z","iopub.status.idle":"2024-01-25T15:09:03.679006Z","shell.execute_reply.started":"2024-01-25T15:09:03.663073Z","shell.execute_reply":"2024-01-25T15:09:03.678193Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"train_data = dataset_tarin.shuffle().map(generate_and_tokenize_prompt)\nval_data = dataset_val.shuffle().map(generate_and_tokenize_prompt)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T15:09:03.680046Z","iopub.execute_input":"2024-01-25T15:09:03.680307Z","iopub.status.idle":"2024-01-25T15:09:19.758711Z","shell.execute_reply.started":"2024-01-25T15:09:03.680280Z","shell.execute_reply":"2024-01-25T15:09:19.757470Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6475 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d346ceda4f6a4e00854e7ce0c331a222"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/970 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bd9d474b70048f996975572c7421678"}},"metadata":{}}]},{"cell_type":"code","source":"def find_all_linear_names(model):\n    \"\"\"\n    Find modules to apply LoRA to.\n\n    :param model: PEFT model\n    \"\"\"\n\n    cls = bnb.nn.Linear4bit\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n\n    if 'lm_head' in lora_module_names:\n        lora_module_names.remove('lm_head')\n    print(f\"LoRA module names: {list(lora_module_names)}\")\n    return list(lora_module_names)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T15:09:19.760191Z","iopub.execute_input":"2024-01-25T15:09:19.760581Z","iopub.status.idle":"2024-01-25T15:09:19.768779Z","shell.execute_reply.started":"2024-01-25T15:09:19.760548Z","shell.execute_reply":"2024-01-25T15:09:19.767549Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"def print_trainable_parameters(model, use_4bit = False):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n\n    :param model: PEFT model\n    \"\"\"\n\n    trainable_params = 0\n    all_param = 0\n\n    for _, param in model.named_parameters():\n        num_params = param.numel()\n        if num_params == 0 and hasattr(param, \"ds_numel\"):\n            num_params = param.ds_numel\n        all_param += num_params\n        if param.requires_grad:\n            trainable_params += num_params\n\n    if use_4bit:\n        trainable_params /= 2\n\n    print(\n        f\"All Parameters: {all_param:,d} || Trainable Parameters: {trainable_params:,d} || Trainable Parameters %: {100 * trainable_params / all_param}\"\n    )","metadata":{"execution":{"iopub.status.busy":"2024-01-25T15:09:19.770336Z","iopub.execute_input":"2024-01-25T15:09:19.770758Z","iopub.status.idle":"2024-01-25T15:09:19.789575Z","shell.execute_reply.started":"2024-01-25T15:09:19.770711Z","shell.execute_reply":"2024-01-25T15:09:19.788301Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"def create_peft_config(r, lora_alpha, target_modules, lora_dropout, bias, task_type):\n    \"\"\"\n    Creates Parameter-Efficient Fine-Tuning configuration for the model\n\n    :param r: LoRA attention dimension\n    :param lora_alpha: Alpha parameter for LoRA scaling\n    :param modules: Names of the modules to apply LoRA to\n    :param lora_dropout: Dropout Probability for LoRA layers\n    :param bias: Specifies if the bias parameters should be trained\n    \"\"\"\n    config = LoraConfig(\n        r = r,\n        lora_alpha = lora_alpha,\n        target_modules = target_modules,\n        lora_dropout = lora_dropout,\n        bias = bias,\n        task_type = task_type,\n    )\n\n    return config","metadata":{"execution":{"iopub.status.busy":"2024-01-25T15:09:19.791079Z","iopub.execute_input":"2024-01-25T15:09:19.791554Z","iopub.status.idle":"2024-01-25T15:09:19.812159Z","shell.execute_reply.started":"2024-01-25T15:09:19.791489Z","shell.execute_reply":"2024-01-25T15:09:19.810739Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def fine_tune(model, tokenizer, dataset, eval_dataset, lora_r, lora_alpha, lora_dropout, bias, task_type, per_device_train_batch_size, gradient_accumulation_steps, warmup_steps, max_steps, num_train_epochs, learning_rate, fp16, logging_steps, output_dir, optim):\n\n    \"\"\"\n    Prepares and fine-tune the pre-trained model.\n\n    :param model: Pre-trained Hugging Face model\n    :param tokenizer: Model tokenizer\n    :param dataset: Preprocessed training dataset\n    \"\"\"\n\n    # Enable gradient checkpointing to reduce memory usage during fine-tuning\n    model.gradient_checkpointing_enable()\n\n    # Prepare the model for training\n    model = prepare_model_for_kbit_training(model)\n\n    # Get LoRA module names\n    target_modules = find_all_linear_names(model)\n\n    # Create PEFT configuration for these modules and wrap the model to PEFT\n    peft_config = create_peft_config(lora_r, lora_alpha, target_modules, lora_dropout, bias, task_type)\n    model = get_peft_model(model, peft_config)\n\n    # Print information about the percentage of trainable parameters\n    print_trainable_parameters(model)\n\n    # Training parameters\n    trainer = Trainer(\n        model = model,\n        train_dataset = dataset,\n        eval_dataset=val_data,\n        args = TrainingArguments(\n            per_device_train_batch_size = per_device_train_batch_size,\n            gradient_accumulation_steps = gradient_accumulation_steps,\n            warmup_steps = warmup_steps,\n#             max_steps = max_steps,\n            num_train_epochs = num_train_epochs,\n            learning_rate = learning_rate,\n            fp16 = fp16,\n            logging_steps = logging_steps,\n            output_dir = output_dir,\n            optim = optim,\n        ),\n        data_collator = transformers.DataCollatorForSeq2Seq(\n            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True)\n        )\n    model.config.use_cache = False\n\n    do_train = True\n\n    # Launch training and log metrics\n    print(\"Training...\")\n\n    trainer.train()\n    model.save_pretrained(output_dir)\n    torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-01-25T15:09:19.814136Z","iopub.execute_input":"2024-01-25T15:09:19.814631Z","iopub.status.idle":"2024-01-25T15:09:19.830087Z","shell.execute_reply.started":"2024-01-25T15:09:19.814586Z","shell.execute_reply":"2024-01-25T15:09:19.828892Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"fine_tune(model, tokenizer, train_data,val_data,lora_r, lora_alpha, lora_dropout, bias, task_type, per_device_train_batch_size, gradient_accumulation_steps, warmup_steps, max_steps, num_train_epochs, learning_rate, fp16, logging_steps, output_dir, optim)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T15:09:19.831768Z","iopub.execute_input":"2024-01-25T15:09:19.832137Z","iopub.status.idle":"2024-01-25T16:26:25.358889Z","shell.execute_reply.started":"2024-01-25T15:09:19.832108Z","shell.execute_reply":"2024-01-25T16:26:25.357774Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"LoRA module names: ['o_proj', 'gate_proj', 'k_proj', 'up_proj', 'down_proj', 'v_proj', 'q_proj']\nAll Parameters: 3,540,414,464 || Trainable Parameters: 39,976,960 || Trainable Parameters %: 1.1291604530062163\nTraining...\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240125_150939-kbz82r65</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/xai_iust/huggingface/runs/kbz82r65' target=\"_blank\">swift-haze-15</a></strong> to <a href='https://wandb.ai/xai_iust/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/xai_iust/huggingface' target=\"_blank\">https://wandb.ai/xai_iust/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/xai_iust/huggingface/runs/kbz82r65' target=\"_blank\">https://wandb.ai/xai_iust/huggingface/runs/kbz82r65</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='259' max='259' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [259/259 1:15:59, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.166300</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3.176300</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.488900</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.565900</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.844900</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.376200</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.062400</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.087500</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.940900</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.776900</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.941700</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.817900</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.751900</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.764100</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.753900</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.693000</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.700900</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.675700</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.636000</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.646100</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.646200</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.659600</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.606200</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.580800</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.670200</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.589400</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.608000</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.615800</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.604500</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.545900</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.560500</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.589700</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.564100</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.544800</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.525100</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.530900</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.548700</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.493600</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.507900</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.522800</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.475400</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.547500</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.462900</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.474500</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.521800</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.524200</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.481800</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.470200</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.456900</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.462100</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>0.384000</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>0.555700</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>0.433700</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>0.399400</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>0.478000</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>0.415900</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>0.453500</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>0.508900</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>0.407900</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.421500</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>0.440300</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>0.390000</td>\n    </tr>\n    <tr>\n      <td>63</td>\n      <td>0.394600</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>0.369100</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>0.422500</td>\n    </tr>\n    <tr>\n      <td>66</td>\n      <td>0.415600</td>\n    </tr>\n    <tr>\n      <td>67</td>\n      <td>0.397800</td>\n    </tr>\n    <tr>\n      <td>68</td>\n      <td>0.411300</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>0.398500</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.364800</td>\n    </tr>\n    <tr>\n      <td>71</td>\n      <td>0.456800</td>\n    </tr>\n    <tr>\n      <td>72</td>\n      <td>0.424200</td>\n    </tr>\n    <tr>\n      <td>73</td>\n      <td>0.393400</td>\n    </tr>\n    <tr>\n      <td>74</td>\n      <td>0.394600</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.443800</td>\n    </tr>\n    <tr>\n      <td>76</td>\n      <td>0.425600</td>\n    </tr>\n    <tr>\n      <td>77</td>\n      <td>0.385500</td>\n    </tr>\n    <tr>\n      <td>78</td>\n      <td>0.392200</td>\n    </tr>\n    <tr>\n      <td>79</td>\n      <td>0.400300</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.363800</td>\n    </tr>\n    <tr>\n      <td>81</td>\n      <td>0.387500</td>\n    </tr>\n    <tr>\n      <td>82</td>\n      <td>0.468400</td>\n    </tr>\n    <tr>\n      <td>83</td>\n      <td>0.444300</td>\n    </tr>\n    <tr>\n      <td>84</td>\n      <td>0.505900</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>0.397300</td>\n    </tr>\n    <tr>\n      <td>86</td>\n      <td>0.393300</td>\n    </tr>\n    <tr>\n      <td>87</td>\n      <td>0.366200</td>\n    </tr>\n    <tr>\n      <td>88</td>\n      <td>0.402100</td>\n    </tr>\n    <tr>\n      <td>89</td>\n      <td>0.398700</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.409500</td>\n    </tr>\n    <tr>\n      <td>91</td>\n      <td>0.285600</td>\n    </tr>\n    <tr>\n      <td>92</td>\n      <td>0.283300</td>\n    </tr>\n    <tr>\n      <td>93</td>\n      <td>0.369300</td>\n    </tr>\n    <tr>\n      <td>94</td>\n      <td>0.382200</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>0.415700</td>\n    </tr>\n    <tr>\n      <td>96</td>\n      <td>0.379000</td>\n    </tr>\n    <tr>\n      <td>97</td>\n      <td>0.387900</td>\n    </tr>\n    <tr>\n      <td>98</td>\n      <td>0.364400</td>\n    </tr>\n    <tr>\n      <td>99</td>\n      <td>0.341500</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.386100</td>\n    </tr>\n    <tr>\n      <td>101</td>\n      <td>0.404200</td>\n    </tr>\n    <tr>\n      <td>102</td>\n      <td>0.350000</td>\n    </tr>\n    <tr>\n      <td>103</td>\n      <td>0.381800</td>\n    </tr>\n    <tr>\n      <td>104</td>\n      <td>0.397200</td>\n    </tr>\n    <tr>\n      <td>105</td>\n      <td>0.402800</td>\n    </tr>\n    <tr>\n      <td>106</td>\n      <td>0.317400</td>\n    </tr>\n    <tr>\n      <td>107</td>\n      <td>0.331000</td>\n    </tr>\n    <tr>\n      <td>108</td>\n      <td>0.396100</td>\n    </tr>\n    <tr>\n      <td>109</td>\n      <td>0.291700</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.428800</td>\n    </tr>\n    <tr>\n      <td>111</td>\n      <td>0.393500</td>\n    </tr>\n    <tr>\n      <td>112</td>\n      <td>0.389900</td>\n    </tr>\n    <tr>\n      <td>113</td>\n      <td>0.312100</td>\n    </tr>\n    <tr>\n      <td>114</td>\n      <td>0.417600</td>\n    </tr>\n    <tr>\n      <td>115</td>\n      <td>0.357800</td>\n    </tr>\n    <tr>\n      <td>116</td>\n      <td>0.451700</td>\n    </tr>\n    <tr>\n      <td>117</td>\n      <td>0.343400</td>\n    </tr>\n    <tr>\n      <td>118</td>\n      <td>0.337000</td>\n    </tr>\n    <tr>\n      <td>119</td>\n      <td>0.357700</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.317400</td>\n    </tr>\n    <tr>\n      <td>121</td>\n      <td>0.274100</td>\n    </tr>\n    <tr>\n      <td>122</td>\n      <td>0.313300</td>\n    </tr>\n    <tr>\n      <td>123</td>\n      <td>0.367600</td>\n    </tr>\n    <tr>\n      <td>124</td>\n      <td>0.301700</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.342400</td>\n    </tr>\n    <tr>\n      <td>126</td>\n      <td>0.260300</td>\n    </tr>\n    <tr>\n      <td>127</td>\n      <td>0.323800</td>\n    </tr>\n    <tr>\n      <td>128</td>\n      <td>0.305700</td>\n    </tr>\n    <tr>\n      <td>129</td>\n      <td>0.376800</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.315000</td>\n    </tr>\n    <tr>\n      <td>131</td>\n      <td>0.332800</td>\n    </tr>\n    <tr>\n      <td>132</td>\n      <td>0.302100</td>\n    </tr>\n    <tr>\n      <td>133</td>\n      <td>0.352700</td>\n    </tr>\n    <tr>\n      <td>134</td>\n      <td>0.392300</td>\n    </tr>\n    <tr>\n      <td>135</td>\n      <td>0.303000</td>\n    </tr>\n    <tr>\n      <td>136</td>\n      <td>0.249200</td>\n    </tr>\n    <tr>\n      <td>137</td>\n      <td>0.368800</td>\n    </tr>\n    <tr>\n      <td>138</td>\n      <td>0.310700</td>\n    </tr>\n    <tr>\n      <td>139</td>\n      <td>0.282400</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.297000</td>\n    </tr>\n    <tr>\n      <td>141</td>\n      <td>0.321500</td>\n    </tr>\n    <tr>\n      <td>142</td>\n      <td>0.299500</td>\n    </tr>\n    <tr>\n      <td>143</td>\n      <td>0.285100</td>\n    </tr>\n    <tr>\n      <td>144</td>\n      <td>0.292100</td>\n    </tr>\n    <tr>\n      <td>145</td>\n      <td>0.293900</td>\n    </tr>\n    <tr>\n      <td>146</td>\n      <td>0.287700</td>\n    </tr>\n    <tr>\n      <td>147</td>\n      <td>0.298800</td>\n    </tr>\n    <tr>\n      <td>148</td>\n      <td>0.303900</td>\n    </tr>\n    <tr>\n      <td>149</td>\n      <td>0.332400</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.316000</td>\n    </tr>\n    <tr>\n      <td>151</td>\n      <td>0.365500</td>\n    </tr>\n    <tr>\n      <td>152</td>\n      <td>0.325800</td>\n    </tr>\n    <tr>\n      <td>153</td>\n      <td>0.307400</td>\n    </tr>\n    <tr>\n      <td>154</td>\n      <td>0.288200</td>\n    </tr>\n    <tr>\n      <td>155</td>\n      <td>0.395800</td>\n    </tr>\n    <tr>\n      <td>156</td>\n      <td>0.188500</td>\n    </tr>\n    <tr>\n      <td>157</td>\n      <td>0.367800</td>\n    </tr>\n    <tr>\n      <td>158</td>\n      <td>0.334600</td>\n    </tr>\n    <tr>\n      <td>159</td>\n      <td>0.332300</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.298300</td>\n    </tr>\n    <tr>\n      <td>161</td>\n      <td>0.293100</td>\n    </tr>\n    <tr>\n      <td>162</td>\n      <td>0.279200</td>\n    </tr>\n    <tr>\n      <td>163</td>\n      <td>0.219300</td>\n    </tr>\n    <tr>\n      <td>164</td>\n      <td>0.254000</td>\n    </tr>\n    <tr>\n      <td>165</td>\n      <td>0.208400</td>\n    </tr>\n    <tr>\n      <td>166</td>\n      <td>0.351100</td>\n    </tr>\n    <tr>\n      <td>167</td>\n      <td>0.306800</td>\n    </tr>\n    <tr>\n      <td>168</td>\n      <td>0.332800</td>\n    </tr>\n    <tr>\n      <td>169</td>\n      <td>0.236900</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.358400</td>\n    </tr>\n    <tr>\n      <td>171</td>\n      <td>0.380700</td>\n    </tr>\n    <tr>\n      <td>172</td>\n      <td>0.308800</td>\n    </tr>\n    <tr>\n      <td>173</td>\n      <td>0.378100</td>\n    </tr>\n    <tr>\n      <td>174</td>\n      <td>0.366700</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>0.349900</td>\n    </tr>\n    <tr>\n      <td>176</td>\n      <td>0.398600</td>\n    </tr>\n    <tr>\n      <td>177</td>\n      <td>0.322300</td>\n    </tr>\n    <tr>\n      <td>178</td>\n      <td>0.231600</td>\n    </tr>\n    <tr>\n      <td>179</td>\n      <td>0.283300</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.284000</td>\n    </tr>\n    <tr>\n      <td>181</td>\n      <td>0.205100</td>\n    </tr>\n    <tr>\n      <td>182</td>\n      <td>0.267900</td>\n    </tr>\n    <tr>\n      <td>183</td>\n      <td>0.288100</td>\n    </tr>\n    <tr>\n      <td>184</td>\n      <td>0.164800</td>\n    </tr>\n    <tr>\n      <td>185</td>\n      <td>0.348900</td>\n    </tr>\n    <tr>\n      <td>186</td>\n      <td>0.254300</td>\n    </tr>\n    <tr>\n      <td>187</td>\n      <td>0.310000</td>\n    </tr>\n    <tr>\n      <td>188</td>\n      <td>0.227300</td>\n    </tr>\n    <tr>\n      <td>189</td>\n      <td>0.189100</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.331600</td>\n    </tr>\n    <tr>\n      <td>191</td>\n      <td>0.327600</td>\n    </tr>\n    <tr>\n      <td>192</td>\n      <td>0.334400</td>\n    </tr>\n    <tr>\n      <td>193</td>\n      <td>0.315500</td>\n    </tr>\n    <tr>\n      <td>194</td>\n      <td>0.264500</td>\n    </tr>\n    <tr>\n      <td>195</td>\n      <td>0.335000</td>\n    </tr>\n    <tr>\n      <td>196</td>\n      <td>0.253300</td>\n    </tr>\n    <tr>\n      <td>197</td>\n      <td>0.267800</td>\n    </tr>\n    <tr>\n      <td>198</td>\n      <td>0.267700</td>\n    </tr>\n    <tr>\n      <td>199</td>\n      <td>0.342500</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.256400</td>\n    </tr>\n    <tr>\n      <td>201</td>\n      <td>0.208800</td>\n    </tr>\n    <tr>\n      <td>202</td>\n      <td>0.263400</td>\n    </tr>\n    <tr>\n      <td>203</td>\n      <td>0.253100</td>\n    </tr>\n    <tr>\n      <td>204</td>\n      <td>0.363200</td>\n    </tr>\n    <tr>\n      <td>205</td>\n      <td>0.257800</td>\n    </tr>\n    <tr>\n      <td>206</td>\n      <td>0.206200</td>\n    </tr>\n    <tr>\n      <td>207</td>\n      <td>0.265700</td>\n    </tr>\n    <tr>\n      <td>208</td>\n      <td>0.266200</td>\n    </tr>\n    <tr>\n      <td>209</td>\n      <td>0.238400</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.379300</td>\n    </tr>\n    <tr>\n      <td>211</td>\n      <td>0.345300</td>\n    </tr>\n    <tr>\n      <td>212</td>\n      <td>0.278300</td>\n    </tr>\n    <tr>\n      <td>213</td>\n      <td>0.224000</td>\n    </tr>\n    <tr>\n      <td>214</td>\n      <td>0.287700</td>\n    </tr>\n    <tr>\n      <td>215</td>\n      <td>0.262300</td>\n    </tr>\n    <tr>\n      <td>216</td>\n      <td>0.208200</td>\n    </tr>\n    <tr>\n      <td>217</td>\n      <td>0.307500</td>\n    </tr>\n    <tr>\n      <td>218</td>\n      <td>0.218400</td>\n    </tr>\n    <tr>\n      <td>219</td>\n      <td>0.208200</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.289100</td>\n    </tr>\n    <tr>\n      <td>221</td>\n      <td>0.219100</td>\n    </tr>\n    <tr>\n      <td>222</td>\n      <td>0.329900</td>\n    </tr>\n    <tr>\n      <td>223</td>\n      <td>0.236500</td>\n    </tr>\n    <tr>\n      <td>224</td>\n      <td>0.193300</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>0.330600</td>\n    </tr>\n    <tr>\n      <td>226</td>\n      <td>0.176800</td>\n    </tr>\n    <tr>\n      <td>227</td>\n      <td>0.259700</td>\n    </tr>\n    <tr>\n      <td>228</td>\n      <td>0.283200</td>\n    </tr>\n    <tr>\n      <td>229</td>\n      <td>0.210600</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.262200</td>\n    </tr>\n    <tr>\n      <td>231</td>\n      <td>0.226400</td>\n    </tr>\n    <tr>\n      <td>232</td>\n      <td>0.231100</td>\n    </tr>\n    <tr>\n      <td>233</td>\n      <td>0.319900</td>\n    </tr>\n    <tr>\n      <td>234</td>\n      <td>0.281500</td>\n    </tr>\n    <tr>\n      <td>235</td>\n      <td>0.248700</td>\n    </tr>\n    <tr>\n      <td>236</td>\n      <td>0.341000</td>\n    </tr>\n    <tr>\n      <td>237</td>\n      <td>0.301100</td>\n    </tr>\n    <tr>\n      <td>238</td>\n      <td>0.282100</td>\n    </tr>\n    <tr>\n      <td>239</td>\n      <td>0.195200</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.206700</td>\n    </tr>\n    <tr>\n      <td>241</td>\n      <td>0.192900</td>\n    </tr>\n    <tr>\n      <td>242</td>\n      <td>0.190700</td>\n    </tr>\n    <tr>\n      <td>243</td>\n      <td>0.244700</td>\n    </tr>\n    <tr>\n      <td>244</td>\n      <td>0.218200</td>\n    </tr>\n    <tr>\n      <td>245</td>\n      <td>0.268900</td>\n    </tr>\n    <tr>\n      <td>246</td>\n      <td>0.227400</td>\n    </tr>\n    <tr>\n      <td>247</td>\n      <td>0.253900</td>\n    </tr>\n    <tr>\n      <td>248</td>\n      <td>0.264400</td>\n    </tr>\n    <tr>\n      <td>249</td>\n      <td>0.268800</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.283900</td>\n    </tr>\n    <tr>\n      <td>251</td>\n      <td>0.286100</td>\n    </tr>\n    <tr>\n      <td>252</td>\n      <td>0.320500</td>\n    </tr>\n    <tr>\n      <td>253</td>\n      <td>0.319000</td>\n    </tr>\n    <tr>\n      <td>254</td>\n      <td>0.209700</td>\n    </tr>\n    <tr>\n      <td>255</td>\n      <td>0.267700</td>\n    </tr>\n    <tr>\n      <td>256</td>\n      <td>0.220100</td>\n    </tr>\n    <tr>\n      <td>257</td>\n      <td>0.231800</td>\n    </tr>\n    <tr>\n      <td>258</td>\n      <td>0.304500</td>\n    </tr>\n    <tr>\n      <td>259</td>\n      <td>0.276300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"login()\n# 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","metadata":{"execution":{"iopub.status.busy":"2024-01-25T16:26:25.362971Z","iopub.execute_input":"2024-01-25T16:26:25.363264Z","iopub.status.idle":"2024-01-25T16:26:25.389087Z","shell.execute_reply.started":"2024-01-25T16:26:25.363239Z","shell.execute_reply":"2024-01-25T16:26:25.388114Z"},"trusted":true},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76bf94b0f137461485fe66d3de1877a2"}},"metadata":{}}]},{"cell_type":"code","source":"api = HfApi()\n\n# task7/task1/QP/no_fine_tune\n# task7/task1/QP/fine_tune/normal_fine_tune/\n# task7/task1/QP/fine_tune/CoT_fine_tune/\n\n\n#Orca_2_7b_task7-1_QP_normal_fine_tune_adapter_config.json\n#Orca_2_7b_task7-1_QP_normal_fine_tune_adapter_model.safetensors\n\n#Orca_2_7b_task7-1_QP_CoT_fine_tune_adapter_config.json\n#Orca_2_7b_task7-1_QP_CoT_fine_tune_adapter_model.safetensors\n\n\n#Orca_2_7b_task7-1_QP_command_fine_tune_test_set_model_responses_headline.json\n#Orca_2_7b_task7-1_QP_command_fine_tune_test_set_true_response_headline.json\n\n#Orca_2_7b_task7-1_QP_CoT_fine_tune_test_set_model_responses_command.json\n#Orca_2_7b_task7-1_QP_CoT_fine_tune_test_set_true_response_command.json\n\n\napi.upload_file(\n    path_or_fileobj=\"/kaggle/working/results/adapter_config.json\",\n    path_in_repo=\"task7/task1/QNLI/fine_tune/normal_fine_tune/fine_tune_on_QP_normal_fine_tune/Orca_2_7b_task7-1_QNLI_normal_fine_tune_on_QP_FT_1epoch_adapter_config.json\",\n    repo_id=\"Mahmood1998/SEM_EVAL_2024\",\n    repo_type=\"model\",\n)\n\napi.upload_file(\n    path_or_fileobj=\"/kaggle/working/results/adapter_model.safetensors\",\n    path_in_repo=\"task7/task1/QNLI/fine_tune/normal_fine_tune/fine_tune_on_QP_normal_fine_tune/Orca_2_7b_task7-1_QNLI_normal_fine_tune_on_QP_FT_1epoch_adapter_model.safetensors\",\n    repo_id=\"Mahmood1998/SEM_EVAL_2024\",\n    repo_type=\"model\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T16:29:12.406658Z","iopub.execute_input":"2024-01-25T16:29:12.407430Z","iopub.status.idle":"2024-01-25T16:29:18.889789Z","shell.execute_reply.started":"2024-01-25T16:29:12.407397Z","shell.execute_reply":"2024-01-25T16:29:18.888720Z"},"trusted":true},"execution_count":35,"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/160M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b996f6a015146ac98c7e57e5dcd34b7"}},"metadata":{}},{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Mahmood1998/SEM_EVAL_2024/commit/1d65c8a44ebc90aeea4a3716956b0ab1a1c9f6e2', commit_message='Upload task7/task1/QNLI/fine_tune/normal_fine_tune/fine_tune_on_QP_normal_fine_tune/Orca_2_7b_task7-1_QNLI_normal_fine_tune_on_QP_FT_1epoch_adapter_model.safetensors with huggingface_hub', commit_description='', oid='1d65c8a44ebc90aeea4a3716956b0ab1a1c9f6e2', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"print(Hi)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T10:42:33.840450Z","iopub.status.idle":"2024-01-24T10:42:33.840924Z","shell.execute_reply.started":"2024-01-24T10:42:33.840682Z","shell.execute_reply":"2024-01-24T10:42:33.840703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lode fine-tune model","metadata":{}},{"cell_type":"code","source":"%pip install accelerate peft bitsandbytes transformers trl","metadata":{"execution":{"iopub.status.busy":"2024-01-24T10:42:33.842371Z","iopub.status.idle":"2024-01-24T10:42:33.842717Z","shell.execute_reply.started":"2024-01-24T10:42:33.842552Z","shell.execute_reply":"2024-01-24T10:42:33.842568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Install dependencies\n!pip install gdown\n!pip install -Uqq gretel-client datasets","metadata":{"execution":{"iopub.status.busy":"2024-01-24T10:42:33.845363Z","iopub.status.idle":"2024-01-24T10:42:33.845977Z","shell.execute_reply.started":"2024-01-24T10:42:33.845720Z","shell.execute_reply":"2024-01-24T10:42:33.845742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import required libraries\nimport textwrap\nimport pandas as pd\nfrom datasets import load_dataset\n\nimport os\nimport json\nimport torch\nimport zipfile\nimport transformers\n\nfrom datasets import Dataset\nfrom huggingface_hub import HfApi\nfrom datasets import load_dataset\nfrom huggingface_hub import login\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel, AutoPeftModelForCausalLM,prepare_model_for_int8_training\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, Trainer, TrainingArguments, DataCollatorForLanguageModeling, EarlyStoppingCallback, pipeline, logging, set_seed, TextStreamer , LlamaTokenizer, StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\n","metadata":{"execution":{"iopub.status.busy":"2024-01-24T10:42:33.847343Z","iopub.status.idle":"2024-01-24T10:42:33.847805Z","shell.execute_reply.started":"2024-01-24T10:42:33.847569Z","shell.execute_reply":"2024-01-24T10:42:33.847590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"login()\n# 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","metadata":{"execution":{"iopub.status.busy":"2024-01-24T10:42:33.849144Z","iopub.status.idle":"2024-01-24T10:42:33.849470Z","shell.execute_reply.started":"2024-01-24T10:42:33.849309Z","shell.execute_reply":"2024-01-24T10:42:33.849325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!gdown --fuzzy -O adapter_config.json https://huggingface.co/Mahmood1998/SEM_EVAL_Task7-1_QP/resolve/main/adapter_config_microsoft/Orca-2-7b_task7.1_QP_1epoch.json?download=true\n!gdown --fuzzy -O adapter_model.safetensors https://huggingface.co/Mahmood1998/SEM_EVAL_Task7-1_QP/resolve/main/adapter_model_microsoft/Orca-2-7b_task7.1_QP_1epoch.safetensors?download=true","metadata":{"execution":{"iopub.status.busy":"2024-01-24T10:42:33.851158Z","iopub.status.idle":"2024-01-24T10:42:33.851477Z","shell.execute_reply.started":"2024-01-24T10:42:33.851314Z","shell.execute_reply":"2024-01-24T10:42:33.851328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = \"microsoft/Orca-2-7b\"\nadapters_name = \"/kaggle/working/\"\n\nprint(f\"Starting to load the model {model_name} into memory\")\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n\nmax_memory = f'{int(torch.cuda.mem_get_info()[0]/1024**3)-1}GB'\nn_gpus = torch.cuda.device_count()\nmax_memory = {i: max_memory for i in range(n_gpus)}\nm = AutoModelForCausalLM.from_pretrained(    \n    model_name,\n    device_map='auto',\n    load_in_8bit=True,\n    max_memory=max_memory,\n    trust_remote_code=True\n)\nprint(\"1\")\nm = PeftModel.from_pretrained(m, adapters_name)\nprint(\"2\")\nm = m.merge_and_unload()\n\ntok = LlamaTokenizer.from_pretrained(model_name)\ntok.bos_token_id = 1\nstop_token_ids = [0]\n\nprint(f\"Successfully loaded the model {model_name} into memory\")","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-01-24T10:42:33.853041Z","iopub.status.idle":"2024-01-24T10:42:33.853370Z","shell.execute_reply.started":"2024-01-24T10:42:33.853209Z","shell.execute_reply":"2024-01-24T10:42:33.853225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modell = m.eval()\nstreamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\ntemplate = (\n    \"A chat between a curious user and an artificial intelligence assistant. \"\n    \"The assistant gives helpful, detailed, and polite answers to the user's questions.\\n\"\n    \"Human: {}\\nAssistant: \"\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T10:42:33.854819Z","iopub.status.idle":"2024-01-24T10:42:33.855256Z","shell.execute_reply.started":"2024-01-24T10:42:33.855083Z","shell.execute_reply":"2024-01-24T10:42:33.855103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!gdown --fuzzy -O NumEval_Task1.zip https://drive.google.com/file/d/1X4Jy0ETkGyaqTlv8luuetLUudd2hcAz1/view?usp=sharing","metadata":{"execution":{"iopub.status.busy":"2024-01-24T10:42:33.856622Z","iopub.status.idle":"2024-01-24T10:42:33.856954Z","shell.execute_reply.started":"2024-01-24T10:42:33.856792Z","shell.execute_reply":"2024-01-24T10:42:33.856807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"zip_file_path = '/kaggle/working/NumEval_Task1.zip'\nextracted_folder_path = '/kaggle/working/NumEval'\n\nwith zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n    zip_ref.extractall(extracted_folder_path)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T10:42:33.858394Z","iopub.status.idle":"2024-01-24T10:42:33.858761Z","shell.execute_reply.started":"2024-01-24T10:42:33.858599Z","shell.execute_reply":"2024-01-24T10:42:33.858615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_of_file_test_QP_command = ['/kaggle/working/NumEval/NumEval_Task1/QP/Numeracy600K_comment_test.json',]\nlist_of_file_test_QP_headline = ['/kaggle/working/NumEval/NumEval_Task1/QP/Numeracy600K_headline_test.json']","metadata":{"execution":{"iopub.status.busy":"2024-01-24T10:42:33.859800Z","iopub.status.idle":"2024-01-24T10:42:33.860102Z","shell.execute_reply.started":"2024-01-24T10:42:33.859950Z","shell.execute_reply":"2024-01-24T10:42:33.859964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def give_prompt_QP_command(comment,mask,answer):\n    prompt = f\"\"\"\nFind the number in the statement and then see the mask and say me which number is masked as answer and don't explain any things and give me resualt in format: {{ Response: answer }} \n### Input:\nstatement:{comment}\nmask:{mask}\"\"\"\n    \n    return {'prompt':prompt,'response':answer}\n\n\ndef give_prompt_QP_headline(title,mask,answer):\n    prompt = f\"\"\" \nFind the number in the statement and then see the mask and say me which number is masked as answer and don't explain any things and give me resualt in format: {{ Response: answer }} \n### Input:\nstatement:{title}\nmask:{mask}\"\"\"\n    \n    return {'prompt':prompt,'response':answer}","metadata":{"execution":{"iopub.status.busy":"2024-01-24T10:42:33.861078Z","iopub.status.idle":"2024-01-24T10:42:33.861379Z","shell.execute_reply.started":"2024-01-24T10:42:33.861227Z","shell.execute_reply":"2024-01-24T10:42:33.861241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-01-24T10:42:33.863187Z","iopub.status.idle":"2024-01-24T10:42:33.863670Z","shell.execute_reply.started":"2024-01-24T10:42:33.863414Z","shell.execute_reply":"2024-01-24T10:42:33.863436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**command test set**","metadata":{}},{"cell_type":"code","source":"list_of_json_lable_test_QP_command = []\nlist_of_text_QP_command = []\nfor file_name in list_of_file_test_QP_command:\n    with open(file_name, 'r') as file:       \n        datas = json.load(file)   \n        for data in datas:\n            dic = give_prompt_QP_command(data['comment'],data['masked'],data['number'])\n            list_of_json_lable_test_QP_command.append(dic['response'])\n            list_of_text_QP_command.append(dic['prompt'])","metadata":{"execution":{"iopub.status.busy":"2024-01-24T10:42:33.864974Z","iopub.status.idle":"2024-01-24T10:42:33.865819Z","shell.execute_reply.started":"2024-01-24T10:42:33.865563Z","shell.execute_reply":"2024-01-24T10:42:33.865588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(list_of_json_lable_Test_QP_command)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T10:42:33.867553Z","iopub.status.idle":"2024-01-24T10:42:33.868001Z","shell.execute_reply.started":"2024-01-24T10:42:33.867764Z","shell.execute_reply":"2024-01-24T10:42:33.867785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"responses = []\ntrue_response = []\n\nbatch_size = 30\n\nfor i in range(0, len(list_of_text_QP_command), batch_size):\n    print(i)\n    prompt_list = list_of_text_QP_command[i:i + batch_size]\n    for j in range(len(prompt_list)):\n        system_message = \"just answer in this format : {'Response' :' Num '}.\"\n        prompt =  f\"<|im_start|>system\\n{system_message}<|im_end|>\\n<|im_start|>user\\n{prompt_list[j]}<|im_end|>\\n<|im_start|>assistant\"\n        prompt_list[j] = prompt\n        \n    true_response = list_of_json_lable_test_QP_command[i:i + batch_size]\n    true_response.append(true_response)\n    inputs_b = tokenizer(prompt_list, return_tensors=\"pt\", padding=True).to(device)\n    input_idsb=inputs_b[\"input_ids\"].to(device)\n    outputs = m.generate(input_idsb, max_new_tokens=15)\n    decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    responses.append(decoded_outputs)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-24T10:42:33.869248Z","iopub.status.idle":"2024-01-24T10:42:33.869710Z","shell.execute_reply.started":"2024-01-24T10:42:33.869459Z","shell.execute_reply":"2024-01-24T10:42:33.869480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_res_test = \"test_set_responses_task7-1_QP_command.json\"\n\nwith open(file_res_test, 'w') as json_file:\n    json.dump(responses, json_file, indent=2)  ","metadata":{"execution":{"iopub.status.busy":"2024-01-24T10:42:33.870827Z","iopub.status.idle":"2024-01-24T10:42:33.871278Z","shell.execute_reply.started":"2024-01-24T10:42:33.871053Z","shell.execute_reply":"2024-01-24T10:42:33.871074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_res_test = \"test_set_true_response_task7-1_QP_command.json\"\n\nwith open(file_res_test, 'w') as json_file:\n    json.dump(true_response, json_file, indent=2) ","metadata":{"execution":{"iopub.status.busy":"2024-01-24T10:42:33.872234Z","iopub.status.idle":"2024-01-24T10:42:33.872716Z","shell.execute_reply.started":"2024-01-24T10:42:33.872443Z","shell.execute_reply":"2024-01-24T10:42:33.872464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"api = HfApi()\n\n# task7/task1/QP/no_fine_tune\n# task7/task1/QP/fine_tune/normal_fine_tune/\n# task7/task1/QP/fine_tune/CoT_fine_tune/\n\n\n#Orca_2_7b_task7-1_QP_normal_fine_tune_adapter_config.json\n#Orca_2_7b_task7-1_QP_normal_fine_tune_adapter_model.safetensors\n\n#Orca_2_7b_task7-1_QP_CoT_fine_tune_adapter_config.json\n#Orca_2_7b_task7-1_QP_CoT_fine_tune_adapter_model.safetensors\n\n\n#Orca_2_7b_task7-1_QP_command_fine_tune_test_set_model_responses_headline.json\n#Orca_2_7b_task7-1_QP_command_fine_tune_test_set_true_response_headline.json\n\n#Orca_2_7b_task7-1_QP_CoT_fine_tune_test_set_model_responses_command.json\n#Orca_2_7b_task7-1_QP_CoT_fine_tune_test_set_true_response_command.json\n\napi.upload_file(\n    path_or_fileobj=\"/kaggle/working/test_set_responses_task7-1_QP_command.json\",\n    path_in_repo=\"URL/Name.json\",\n    repo_id=\"Mahmood1998/SEM_EVAL_2024\",\n    repo_type=\"model\",\n)\n\n\napi.upload_file(\n    path_or_fileobj=\"/kaggle/working/test_set_true_response_task7-1_QP_command.json\",\n    path_in_repo=\"URL/Name.json\",\n    repo_id=\"Mahmood1998/test\",\n    repo_type=\"model\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T10:42:33.873775Z","iopub.status.idle":"2024-01-24T10:42:33.874105Z","shell.execute_reply.started":"2024-01-24T10:42:33.873941Z","shell.execute_reply":"2024-01-24T10:42:33.873957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**headline test set**","metadata":{}},{"cell_type":"code","source":"list_of_json_lable_test_QP_headline = []\nlist_of_text_QP_headline = []\n\nfor file_name in list_of_file_test_QP_headline:\n    with open(file_name, 'r') as file: \n        datas = json.load(file)\n        for data in datas:\n            dic = give_prompt_QP_headline(data['title'],data['masked'],data['number'])\n            list_of_json_lable_test_QP_headline.append(dic['response'])\n            list_of_text_QP_headline.append(dic['prompt'])","metadata":{"execution":{"iopub.status.busy":"2024-01-24T10:42:33.875974Z","iopub.status.idle":"2024-01-24T10:42:33.876286Z","shell.execute_reply.started":"2024-01-24T10:42:33.876131Z","shell.execute_reply":"2024-01-24T10:42:33.876146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"responses = []\ntrue_response = []\n\nbatch_size = 30\n\nfor i in range(0, len(list_of_text_QP_headline), batch_size):\n    print(i)\n    prompt_list = list_of_text_QP_headline[i:i + batch_size]\n    for j in range(len(prompt_list)):\n        system_message = \"just answer in this format : {'Response' :' Num '}.\"\n        prompt =  f\"<|im_start|>system\\n{system_message}<|im_end|>\\n<|im_start|>user\\n{prompt_list[j]}<|im_end|>\\n<|im_start|>assistant\"\n        prompt_list[j] = prompt\n        \n    true_response = list_of_json_lable_test_QP_headline[i:i + batch_size]\n    true_response.append(true_response)\n    inputs_b = tokenizer(prompt_list, return_tensors=\"pt\", padding=True).to(device)\n    input_idsb=inputs_b[\"input_ids\"].to(device)\n    outputs = m.generate(input_idsb, max_new_tokens=15)\n    decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    responses.append(decoded_outputs)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T10:42:33.877858Z","iopub.status.idle":"2024-01-24T10:42:33.878174Z","shell.execute_reply.started":"2024-01-24T10:42:33.878015Z","shell.execute_reply":"2024-01-24T10:42:33.878030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_res_test = \"test_set_model_responses_task7-1_QP_headline.json\"\n\nwith open(file_res_test, 'w') as json_file:\n    json.dump(responses, json_file, indent=2)  ","metadata":{"execution":{"iopub.status.busy":"2024-01-24T10:42:33.879289Z","iopub.status.idle":"2024-01-24T10:42:33.879656Z","shell.execute_reply.started":"2024-01-24T10:42:33.879461Z","shell.execute_reply":"2024-01-24T10:42:33.879478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_res_test = \"test_set_true_response_task7-1_QP_headline.json\"\n\nwith open(file_res_test, 'w') as json_file:\n    json.dump(true_response, json_file, indent=2) ","metadata":{"execution":{"iopub.status.busy":"2024-01-24T10:42:33.880806Z","iopub.status.idle":"2024-01-24T10:42:33.881126Z","shell.execute_reply.started":"2024-01-24T10:42:33.880971Z","shell.execute_reply":"2024-01-24T10:42:33.880985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"api = HfApi()\n\n# task7/task1/QP/no_fine_tune\n# task7/task1/QP/fine_tune/normal_fine_tune/\n# task7/task1/QP/fine_tune/CoT_fine_tune/\n\n\n#Orca_2_7b_task7-1_QP_normal_fine_tune_adapter_config.json\n#Orca_2_7b_task7-1_QP_normal_fine_tune_adapter_model.safetensors\n\n#Orca_2_7b_task7-1_QP_CoT_fine_tune_adapter_config.json\n#Orca_2_7b_task7-1_QP_CoT_fine_tune_adapter_model.safetensors\n\n\n#Orca_2_7b_task7-1_QP_command_fine_tune_test_set_model_responses_headline.json\n#Orca_2_7b_task7-1_QP_command_fine_tune_test_set_true_response_headline.json\n\n#Orca_2_7b_task7-1_QP_CoT_fine_tune_test_set_model_responses_command.json\n#Orca_2_7b_task7-1_QP_CoT_fine_tune_test_set_true_response_command.json\n\napi.upload_file(\n    path_or_fileobj=\"/kaggle/working/test_set_model_responses_task7-1_QP_headline.json\",\n    path_in_repo=\"URL/Name.json\",\n    repo_id=\"Mahmood1998/SEM_EVAL_2024\",\n    repo_type=\"model\",\n)\n\n\napi.upload_file(\n    path_or_fileobj=\"/kaggle/working/test_set_true_response_task7-1_QP_headline.json\",\n    path_in_repo=\"URL/Name.json\",\n    repo_id=\"Mahmood1998/test\",\n    repo_type=\"model\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T10:42:33.883177Z","iopub.status.idle":"2024-01-24T10:42:33.883660Z","shell.execute_reply.started":"2024-01-24T10:42:33.883405Z","shell.execute_reply":"2024-01-24T10:42:33.883429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}